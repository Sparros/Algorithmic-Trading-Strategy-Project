{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "cb194745",
      "metadata": {},
      "source": [
        "# Dataset Screening — Fast Triage\n",
        "\n",
        "This notebook screens all `{sector}__{ticker}.csv` files for:\n",
        "- Sample depth & continuity\n",
        "- Target viability\n",
        "- Missingness\n",
        "- Feature diversity\n",
        "- **Preliminary signal (fast Pearson IC, with optional Spearman confirm)**\n",
        "- Regime robustness\n",
        "- Leakage / live-feasibility checks\n",
        "\n",
        "It then scores and ranks datasets, and writes `dataset_screen_report.json`.\n",
        "\n",
        "### Speed-up strategies in this version\n",
        "- Two-stage IC: **fast rolling Pearson on winsorized z-scores** over recent history, then **Spearman confirm** only on promoted features (default top 30 or abs(medIC) ≥ 0.01).\n",
        "- Trim feature set before IC (regex + variance + correlation reps).\n",
        "- Reuse the fast pass wherever possible.\n",
        "\n",
        "You can tune the speed/rigor tradeoff in the **Configuration** cell.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ddd47e56",
      "metadata": {},
      "outputs": [],
      "source": [
        "import os, sys, glob, time, math, re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
        "\n",
        "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..')))\n",
        "\n",
        "from src.modelling_functions import create_target_variable\n",
        "\n",
        "# Path where your processed datasets live (Windows raw string recommended)\n",
        "DATA_DIR = r\"C:\\Users\\epoch_bpjmdqk\\Documents\\Code\\data\\processed\"\n",
        "\n",
        "# Triage knobs — tweak freely\n",
        "RECENT_ROWS = 750           # use only the most recent N rows in wide triage\n",
        "MIN_SAMPLES = 150           # minimum aligned samples to score a feature\n",
        "MAX_NANS = 0.3              # drop features with >30% missing after alignment\n",
        "TOP_FEATURES_PER_SET = 5    # aggregate score: mean of top-K feature scores\n",
        "TOP_DATASETS_TO_KEEP = 30   # how many datasets to pass to triage_fine\n",
        "\n",
        "# Target generation knobs (used via your create_target_variable)\n",
        "TARGET_WINDOW = 1\n",
        "TARGET_THRESHOLD = 0.0\n",
        "\n",
        "# Output\n",
        "RESULTS_CSV = os.path.join(DATA_DIR, \"triage_wide_results.csv\")\n",
        "TOPLIST_CSV = os.path.join(DATA_DIR, \"triage_wide_toplist.csv\")\n",
        "\n",
        "CANDIDATE_PATHS = [\n",
        "    os.getcwd(),\n",
        "    os.path.abspath(os.path.join(os.getcwd(), \"..\")),\n",
        "    os.path.abspath(os.path.join(os.path.dirname(DATA_DIR), \"..\")),  # project root guess\n",
        "]\n",
        "for p in CANDIDATE_PATHS:\n",
        "    if p not in sys.path and os.path.isdir(p):\n",
        "        sys.path.append(p)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "fc769834",
      "metadata": {},
      "outputs": [],
      "source": [
        "def parse_sector_ticker(filename):\n",
        "    # expects format {sector}__{ticker}.csv (as in staples__WMT.csv)\n",
        "    base = os.path.basename(filename)\n",
        "    name, _ = os.path.splitext(base)\n",
        "    if \"__\" in name:\n",
        "        sector, ticker = name.split(\"__\", 1)\n",
        "    else:\n",
        "        # fallback if single underscore was used\n",
        "        parts = name.split(\"_\")\n",
        "        sector, ticker = parts[0], parts[-1]\n",
        "    return sector, ticker\n",
        "\n",
        "def _point_biserial_fast(x, y_bin):\n",
        "    \"\"\"\n",
        "    Pearson correlation between numeric x and binary y (0/1).\n",
        "    Returns absolute value; NaN if insufficient variance.\n",
        "    \"\"\"\n",
        "    if x.size < MIN_SAMPLES:\n",
        "        return np.nan\n",
        "    if (y_bin.max() == y_bin.min()):\n",
        "        return np.nan  # constant target\n",
        "    # standardize to avoid overflow\n",
        "    x = x.astype(np.float64)\n",
        "    y = y_bin.astype(np.float64)\n",
        "    x_mean = np.nanmean(x)\n",
        "    y_mean = np.nanmean(y)\n",
        "    x_std = np.nanstd(x)\n",
        "    y_std = np.nanstd(y)\n",
        "    if x_std == 0 or y_std == 0 or np.isnan(x_std) or np.isnan(y_std):\n",
        "        return np.nan\n",
        "    cov = np.nanmean((x - x_mean) * (y - y_mean))\n",
        "    r = cov / (x_std * y_std)\n",
        "    return abs(r)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "d9ca41a5",
      "metadata": {},
      "outputs": [],
      "source": [
        "def score_dataset(file_path):\n",
        "    sector, ticker = parse_sector_ticker(file_path)\n",
        "\n",
        "    # Load\n",
        "    try:\n",
        "        # parse_dates is optional if Date exists; use dtype downcasting later\n",
        "        df = pd.read_csv(file_path)\n",
        "    except Exception as e:\n",
        "        return {\n",
        "            \"dataset\": os.path.basename(file_path),\n",
        "            \"sector\": sector,\n",
        "            \"ticker\": ticker,\n",
        "            \"score\": np.nan,\n",
        "            \"n_rows\": 0,\n",
        "            \"pos_rate\": np.nan,\n",
        "            \"n_features_scored\": 0,\n",
        "            \"error\": f\"read_error: {e}\",\n",
        "        }\n",
        "\n",
        "    # Ensure Close column exists\n",
        "    close_col = f\"Close_{ticker}\"\n",
        "    if close_col not in df.columns:\n",
        "        # try common alternatives\n",
        "        alt = \"Close\"\n",
        "        if alt in df.columns:\n",
        "            df.rename(columns={alt: close_col}, inplace=True)\n",
        "        else:\n",
        "            return {\n",
        "                \"dataset\": os.path.basename(file_path),\n",
        "                \"sector\": sector,\n",
        "                \"ticker\": ticker,\n",
        "                \"score\": np.nan,\n",
        "                \"n_rows\": len(df),\n",
        "                \"pos_rate\": np.nan,\n",
        "                \"n_features_scored\": 0,\n",
        "                \"error\": f\"missing_close_col ({close_col})\",\n",
        "            }\n",
        "\n",
        "    # Sort by date if present\n",
        "    if \"Date\" in df.columns:\n",
        "        try:\n",
        "            df[\"Date\"] = pd.to_datetime(df[\"Date\"], errors=\"coerce\")\n",
        "            df = df.sort_values(\"Date\")\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "    # Use only recent rows for speed\n",
        "    if RECENT_ROWS and len(df) > RECENT_ROWS:\n",
        "        df = df.tail(RECENT_ROWS).copy()\n",
        "\n",
        "    # Create target using your function\n",
        "    try:\n",
        "        df = create_target_variable(df, ticker=ticker, window=TARGET_WINDOW, threshold=TARGET_THRESHOLD)\n",
        "    except Exception as e:\n",
        "        return {\n",
        "            \"dataset\": os.path.basename(file_path),\n",
        "            \"sector\": sector,\n",
        "            \"ticker\": ticker,\n",
        "            \"score\": np.nan,\n",
        "            \"n_rows\": len(df),\n",
        "            \"pos_rate\": np.nan,\n",
        "            \"n_features_scored\": 0,\n",
        "            \"error\": f\"target_error: {e}\",\n",
        "        }\n",
        "\n",
        "    target_col = f\"{ticker}_Target\"\n",
        "    ret_col = f\"{ticker}_target_return_{TARGET_WINDOW}D_{TARGET_THRESHOLD}\"\n",
        "    if target_col not in df.columns:\n",
        "        return {\n",
        "            \"dataset\": os.path.basename(file_path),\n",
        "            \"sector\": sector,\n",
        "            \"ticker\": ticker,\n",
        "            \"score\": np.nan,\n",
        "            \"n_rows\": len(df),\n",
        "            \"pos_rate\": np.nan,\n",
        "            \"n_features_scored\": 0,\n",
        "            \"error\": f\"missing_target_col ({target_col})\",\n",
        "        }\n",
        "\n",
        "    # Prepare candidate features\n",
        "    exclude = {target_col, ret_col, \"Date\"}\n",
        "    feature_cols = [c for c in df.columns\n",
        "                    if c not in exclude and df[c].dtype != \"O\" and not c.endswith(\"_Target\")]\n",
        "    if not feature_cols:\n",
        "        return {\n",
        "            \"dataset\": os.path.basename(file_path),\n",
        "            \"sector\": sector,\n",
        "            \"ticker\": ticker,\n",
        "            \"score\": np.nan,\n",
        "            \"n_rows\": len(df),\n",
        "            \"pos_rate\": df[target_col].mean() if target_col in df else np.nan,\n",
        "            \"n_features_scored\": 0,\n",
        "            \"error\": \"no_numeric_features\",\n",
        "        }\n",
        "\n",
        "    y = df[target_col].astype(\"float32\").to_numpy()\n",
        "    scores = []\n",
        "\n",
        "    # Score each feature with fast point-biserial correlation vs binary target\n",
        "    for col in feature_cols:\n",
        "        x = pd.to_numeric(df[col], errors=\"coerce\").to_numpy()\n",
        "        mask = ~np.isnan(x) & ~np.isnan(y)\n",
        "        if mask.sum() < MIN_SAMPLES:\n",
        "            continue\n",
        "        x_m = x[mask]\n",
        "        y_m = y[mask]\n",
        "        # drop columns with too many NaNs after alignment\n",
        "        if 1 - (mask.sum() / len(mask)) > MAX_NANS:\n",
        "            continue\n",
        "        r = _point_biserial_fast(x_m, y_m)\n",
        "        if not np.isnan(r):\n",
        "            scores.append((col, r))\n",
        "\n",
        "    if not scores:\n",
        "        return {\n",
        "            \"dataset\": os.path.basename(file_path),\n",
        "            \"sector\": sector,\n",
        "            \"ticker\": ticker,\n",
        "            \"score\": np.nan,\n",
        "            \"n_rows\": len(df),\n",
        "            \"pos_rate\": float(np.nanmean(y)) if y.size else np.nan,\n",
        "            \"n_features_scored\": 0,\n",
        "            \"error\": \"no_scored_features\",\n",
        "        }\n",
        "\n",
        "    # Aggregate: mean of top-K absolute correlations\n",
        "    scores.sort(key=lambda t: t[1], reverse=True)\n",
        "    top_k = scores[:TOP_FEATURES_PER_SET]\n",
        "    agg_score = float(np.mean([s for _, s in top_k]))\n",
        "\n",
        "    return {\n",
        "        \"dataset\": os.path.basename(file_path),\n",
        "        \"sector\": sector,\n",
        "        \"ticker\": ticker,\n",
        "        \"score\": agg_score,\n",
        "        \"n_rows\": len(df),\n",
        "        \"pos_rate\": float(np.nanmean(y)) if y.size else np.nan,\n",
        "        \"n_features_scored\": len(scores),\n",
        "        \"top_features\": [c for c, _ in top_k],\n",
        "        \"error\": \"\",\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "28d17346",
      "metadata": {},
      "outputs": [],
      "source": [
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "\n",
        "def run_wide_triage():\n",
        "    all_files = glob.glob(os.path.join(DATA_DIR, \"*.csv\"))\n",
        "    if not all_files:\n",
        "        raise FileNotFoundError(f\"No CSVs found in {DATA_DIR}\")\n",
        "\n",
        "    start = time.time()\n",
        "    results = []\n",
        "\n",
        "    # Good default for I/O-bound work\n",
        "    max_workers = min(32, (os.cpu_count() or 4) * 4)\n",
        "\n",
        "    def _safe_score(path):\n",
        "        try:\n",
        "            return score_dataset(path)\n",
        "        except Exception as e:\n",
        "            # Ensure a single bad file doesn't kill the batch\n",
        "            sector, ticker = parse_sector_ticker(path)\n",
        "            return {\n",
        "                \"dataset\": os.path.basename(path),\n",
        "                \"sector\": sector,\n",
        "                \"ticker\": ticker,\n",
        "                \"score\": np.nan,\n",
        "                \"n_rows\": 0,\n",
        "                \"pos_rate\": np.nan,\n",
        "                \"n_features_scored\": 0,\n",
        "                \"error\": f\"worker_exception: {type(e).__name__}: {e}\",\n",
        "            }\n",
        "\n",
        "    with ThreadPoolExecutor(max_workers=max_workers) as ex:\n",
        "        fut2file = {ex.submit(_safe_score, f): f for f in all_files}\n",
        "        for fut in as_completed(fut2file):\n",
        "            res = fut.result()  # _safe_score already guards exceptions\n",
        "            results.append(res)\n",
        "\n",
        "    dfres = pd.DataFrame(results)\n",
        "    # If everything failed, be explicit\n",
        "    if dfres[\"score\"].notna().sum() == 0:\n",
        "        print(\"Warning: all datasets failed; check 'error' column for details.\")\n",
        "    dfres = dfres.sort_values([\"score\"], ascending=[False], na_position=\"last\")\n",
        "    dfres.to_csv(RESULTS_CSV, index=False)\n",
        "\n",
        "    toplist = dfres.head(TOP_DATASETS_TO_KEEP).copy()\n",
        "    toplist[[\"dataset\", \"sector\", \"ticker\", \"score\"]].to_csv(TOPLIST_CSV, index=False)\n",
        "\n",
        "    elapsed = time.time() - start\n",
        "    print(f\"Triage complete in {elapsed:,.1f}s\")\n",
        "    print(f\"Saved: {RESULTS_CSV}\")\n",
        "    print(f\"Saved top {TOP_DATASETS_TO_KEEP}: {TOPLIST_CSV}\")\n",
        "    return dfres, toplist"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}

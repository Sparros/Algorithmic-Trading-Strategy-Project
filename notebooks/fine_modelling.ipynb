{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c249f71c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..')))\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import os # Added to create the directory\n",
    "from sklearn.model_selection import GridSearchCV, TimeSeriesSplit\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, f1_score, precision_score, recall_score, accuracy_score\n",
    "from xgboost import XGBClassifier\n",
    "from catboost import CatBoostClassifier # Assuming this library is installed\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from collections import defaultdict\n",
    "from src.stock_features import create_target_variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "59a5d6d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(r'C:\\Users\\epoch_bpjmdqk\\Documents\\Code\\data\\processed\\stock_and_macro.csv', index_col='Date', parse_dates=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e9801717",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Data setup for Window=10, Threshold=0.005 ---\n",
      "Class imbalance ratio (0/1): 0.97\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "# We'll focus on the best-performing data preparation parameters\n",
    "window = 10\n",
    "threshold = 0.005\n",
    "target_ticker = 'WMT'\n",
    "split_date = '2021-01-01'\n",
    "\n",
    "data_target = create_target_variable(data.copy(), target_ticker, window=window, threshold=threshold)\n",
    "\n",
    "target_col_name = f'{target_ticker}_Target'\n",
    "target_return_col_name = f'{target_ticker}_target_return_{window}D_{threshold}'\n",
    "\n",
    "columns_to_drop = [\n",
    "    target_col_name,\n",
    "    target_return_col_name,\n",
    "    f'Open_{target_ticker}',\n",
    "    f'High_{target_ticker}',\n",
    "    f'Low_{target_ticker}',\n",
    "    f'Close_{target_ticker}'\n",
    "]\n",
    "\n",
    "# Handle NaN values and split the data\n",
    "data_target.dropna(inplace=True)\n",
    "X = data_target.drop(columns=columns_to_drop)\n",
    "y = data_target[target_col_name]\n",
    "\n",
    "# Create a fixed training set for the final model\n",
    "X_train_full = X.loc[:split_date].copy()\n",
    "y_train_full = y.loc[:split_date].copy()\n",
    "\n",
    "# Create a fixed testing set for the final evaluation\n",
    "X_test_full = X.loc[split_date:].copy()\n",
    "y_test_full = y.loc[split_date:].copy()\n",
    "\n",
    "neg_to_pos_ratio = (y_train_full == 0).sum() / (y_train_full == 1).sum()\n",
    "print(f\"\\n--- Data setup for Window={window}, Threshold={threshold} ---\")\n",
    "print(f\"Class imbalance ratio (0/1): {neg_to_pos_ratio:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c15eb0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The 'C' parameter in Logistic Regression is the inverse of regularization strength.\n",
    "# A smaller value of C means stronger regularization. We'll search a wider,\n",
    "# more granular range to find the optimal value.\n",
    "refined_experiment_configs = [\n",
    "    # Logistic Regression: Good baseline model.\n",
    "    {\n",
    "        'model_name': 'LogisticRegression',\n",
    "        'model_class': LogisticRegression,\n",
    "        'initial_params': {'random_state': 42, 'class_weight': 'balanced'},\n",
    "        # Refined param grid based on coarse search results\n",
    "        'param_grid': {\n",
    "            'C': [0.05, 0.1, 0.5, 1.0],\n",
    "            'solver': ['liblinear']\n",
    "        }\n",
    "    },\n",
    "    # XGBoost: A powerful, gradient-boosting model often used for tabular data.\n",
    "    {\n",
    "        'model_name': 'XGBoost',\n",
    "        'model_class': XGBClassifier,\n",
    "        'initial_params': {'eval_metric': 'logloss', 'use_label_encoder': False, 'random_state': 42, 'scale_pos_weight': neg_to_pos_ratio},\n",
    "        # Refined param grid based on coarse search results\n",
    "        'param_grid': {\n",
    "            'n_estimators': [75, 100, 125, 150],\n",
    "            'learning_rate': [0.08, 0.1, 0.12],\n",
    "            'max_depth': [3, 4],\n",
    "            'subsample': [0.8, 1.0],\n",
    "            'colsample_bytree': [0.8, 1.0]\n",
    "        }\n",
    "    },\n",
    "    # CatBoost: Another robust gradient-boosting model that handles categorical features automatically.\n",
    "    {\n",
    "        'model_name': 'CatBoost',\n",
    "        'model_class': CatBoostClassifier,\n",
    "        'initial_params': {'verbose': False, 'random_state': 42, 'early_stopping_rounds': 50, 'class_weights': [1, neg_to_pos_ratio]},\n",
    "        # Refined param grid based on coarse search results\n",
    "        'param_grid': {\n",
    "            'n_estimators': [75, 100, 125, 150],\n",
    "            'learning_rate': [0.08, 0.1, 0.12],\n",
    "            'depth': [3, 4]\n",
    "        }\n",
    "    },\n",
    "    # RandomForest: An ensemble tree method that is often less prone to overfitting.\n",
    "    {\n",
    "        'model_name': 'RandomForest',\n",
    "        'model_class': RandomForestClassifier,\n",
    "        'initial_params': {'random_state': 42, 'class_weight': 'balanced'},\n",
    "        # Refined param grid based on coarse search results\n",
    "        'param_grid': {\n",
    "            'n_estimators': [100, 150, 200, 250],\n",
    "            'max_depth': [4, 5, 6],\n",
    "            'min_samples_split': [2, 3],\n",
    "            'min_samples_leaf': [1, 2]\n",
    "        }\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7be48f4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Starting Time-Series Cross-Validation for LogisticRegression ---\n",
      "Fold 1/5: Training on 455 samples, testing on 453 samples\n",
      "Fold 2/5: Training on 908 samples, testing on 453 samples\n",
      "Fold 3/5: Training on 1361 samples, testing on 453 samples\n",
      "Fold 4/5: Training on 1814 samples, testing on 453 samples\n",
      "Fold 5/5: Training on 2267 samples, testing on 453 samples\n",
      "\n",
      "--- Cross-Validation Results Summary ---\n",
      "Average accuracy: 0.4645 (+/- 0.0324)\n",
      "Average precision: 0.4062 (+/- 0.2136)\n",
      "Average recall: 0.3651 (+/- 0.3662)\n",
      "Average f1: 0.3118 (+/- 0.2305)\n",
      "Best params per fold: [{'C': np.float64(0.001), 'solver': 'liblinear'}, {'C': np.float64(0.01), 'solver': 'liblinear'}, {'C': np.float64(10.0), 'solver': 'liblinear'}, {'C': np.float64(100.0), 'solver': 'liblinear'}, {'C': np.float64(1.0), 'solver': 'liblinear'}]\n",
      "\n",
      "--- Starting Time-Series Cross-Validation for XGBoost ---\n",
      "Fold 1/5: Training on 455 samples, testing on 453 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\epoch_bpjmdqk\\Documents\\Code\\venv\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [14:06:40] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2/5: Training on 908 samples, testing on 453 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\epoch_bpjmdqk\\Documents\\Code\\venv\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [14:09:16] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3/5: Training on 1361 samples, testing on 453 samples\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[30]\u001b[39m\u001b[32m, line 29\u001b[39m\n\u001b[32m     21\u001b[39m \u001b[38;5;66;03m# Perform grid search for this specific fold\u001b[39;00m\n\u001b[32m     22\u001b[39m grid_search_fold = GridSearchCV(\n\u001b[32m     23\u001b[39m     estimator=model_class(**initial_params),\n\u001b[32m     24\u001b[39m     param_grid=param_grid,\n\u001b[32m   (...)\u001b[39m\u001b[32m     27\u001b[39m     n_jobs=-\u001b[32m1\u001b[39m\n\u001b[32m     28\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m29\u001b[39m \u001b[43mgrid_search_fold\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     30\u001b[39m best_model_fold = grid_search_fold.best_estimator_\n\u001b[32m     32\u001b[39m \u001b[38;5;66;03m# Evaluate the best model from this fold on the test split for this fold\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\epoch_bpjmdqk\\Documents\\Code\\venv\\Lib\\site-packages\\sklearn\\base.py:1365\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1358\u001b[39m     estimator._validate_params()\n\u001b[32m   1360\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1361\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1362\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1363\u001b[39m     )\n\u001b[32m   1364\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1365\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\epoch_bpjmdqk\\Documents\\Code\\venv\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1051\u001b[39m, in \u001b[36mBaseSearchCV.fit\u001b[39m\u001b[34m(self, X, y, **params)\u001b[39m\n\u001b[32m   1045\u001b[39m     results = \u001b[38;5;28mself\u001b[39m._format_results(\n\u001b[32m   1046\u001b[39m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[32m   1047\u001b[39m     )\n\u001b[32m   1049\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[32m-> \u001b[39m\u001b[32m1051\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_run_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevaluate_candidates\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1053\u001b[39m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[32m   1054\u001b[39m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[32m   1055\u001b[39m first_test_score = all_out[\u001b[32m0\u001b[39m][\u001b[33m\"\u001b[39m\u001b[33mtest_scores\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\epoch_bpjmdqk\\Documents\\Code\\venv\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1605\u001b[39m, in \u001b[36mGridSearchCV._run_search\u001b[39m\u001b[34m(self, evaluate_candidates)\u001b[39m\n\u001b[32m   1603\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_run_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, evaluate_candidates):\n\u001b[32m   1604\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1605\u001b[39m     \u001b[43mevaluate_candidates\u001b[49m\u001b[43m(\u001b[49m\u001b[43mParameterGrid\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mparam_grid\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\epoch_bpjmdqk\\Documents\\Code\\venv\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:997\u001b[39m, in \u001b[36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[39m\u001b[34m(candidate_params, cv, more_results)\u001b[39m\n\u001b[32m    989\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.verbose > \u001b[32m0\u001b[39m:\n\u001b[32m    990\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[32m    991\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mFitting \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[33m folds for each of \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[33m candidates,\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    992\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m totalling \u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[33m fits\u001b[39m\u001b[33m\"\u001b[39m.format(\n\u001b[32m    993\u001b[39m             n_splits, n_candidates, n_candidates * n_splits\n\u001b[32m    994\u001b[39m         )\n\u001b[32m    995\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m997\u001b[39m out = \u001b[43mparallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    998\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_fit_and_score\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    999\u001b[39m \u001b[43m        \u001b[49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_estimator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1000\u001b[39m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1001\u001b[39m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1002\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1003\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1004\u001b[39m \u001b[43m        \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1005\u001b[39m \u001b[43m        \u001b[49m\u001b[43msplit_progress\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_splits\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1006\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcandidate_progress\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_candidates\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1007\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mfit_and_score_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1008\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1009\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mproduct\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1010\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcandidate_params\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1011\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcv\u001b[49m\u001b[43m.\u001b[49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mrouted_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43msplitter\u001b[49m\u001b[43m.\u001b[49m\u001b[43msplit\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1012\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1013\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1015\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) < \u001b[32m1\u001b[39m:\n\u001b[32m   1016\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1017\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mNo fits were performed. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1018\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mWas the CV iterator empty? \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1019\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mWere there no candidates?\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1020\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\epoch_bpjmdqk\\Documents\\Code\\venv\\Lib\\site-packages\\sklearn\\utils\\parallel.py:82\u001b[39m, in \u001b[36mParallel.__call__\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m     73\u001b[39m warning_filters = warnings.filters\n\u001b[32m     74\u001b[39m iterable_with_config_and_warning_filters = (\n\u001b[32m     75\u001b[39m     (\n\u001b[32m     76\u001b[39m         _with_config_and_warning_filters(delayed_func, config, warning_filters),\n\u001b[32m   (...)\u001b[39m\u001b[32m     80\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[32m     81\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m82\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config_and_warning_filters\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\epoch_bpjmdqk\\Documents\\Code\\venv\\Lib\\site-packages\\joblib\\parallel.py:2072\u001b[39m, in \u001b[36mParallel.__call__\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m   2066\u001b[39m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[32m   2067\u001b[39m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[32m   2068\u001b[39m \u001b[38;5;66;03m# reaches the first `yield` statement. This starts the asynchronous\u001b[39;00m\n\u001b[32m   2069\u001b[39m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[32m   2070\u001b[39m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[32m-> \u001b[39m\u001b[32m2072\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.return_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\epoch_bpjmdqk\\Documents\\Code\\venv\\Lib\\site-packages\\joblib\\parallel.py:1682\u001b[39m, in \u001b[36mParallel._get_outputs\u001b[39m\u001b[34m(self, iterator, pre_dispatch)\u001b[39m\n\u001b[32m   1679\u001b[39m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[32m   1681\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backend.retrieval_context():\n\u001b[32m-> \u001b[39m\u001b[32m1682\u001b[39m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m._retrieve()\n\u001b[32m   1684\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[32m   1685\u001b[39m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[32m   1686\u001b[39m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[32m   1687\u001b[39m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[32m   1688\u001b[39m     \u001b[38;5;28mself\u001b[39m._exception = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\epoch_bpjmdqk\\Documents\\Code\\venv\\Lib\\site-packages\\joblib\\parallel.py:1800\u001b[39m, in \u001b[36mParallel._retrieve\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1789\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.return_ordered:\n\u001b[32m   1790\u001b[39m     \u001b[38;5;66;03m# Case ordered: wait for completion (or error) of the next job\u001b[39;00m\n\u001b[32m   1791\u001b[39m     \u001b[38;5;66;03m# that have been dispatched and not retrieved yet. If no job\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1795\u001b[39m     \u001b[38;5;66;03m# control only have to be done on the amount of time the next\u001b[39;00m\n\u001b[32m   1796\u001b[39m     \u001b[38;5;66;03m# dispatched job is pending.\u001b[39;00m\n\u001b[32m   1797\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m (nb_jobs == \u001b[32m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[32m   1798\u001b[39m         \u001b[38;5;28mself\u001b[39m._jobs[\u001b[32m0\u001b[39m].get_status(timeout=\u001b[38;5;28mself\u001b[39m.timeout) == TASK_PENDING\n\u001b[32m   1799\u001b[39m     ):\n\u001b[32m-> \u001b[39m\u001b[32m1800\u001b[39m         \u001b[43mtime\u001b[49m\u001b[43m.\u001b[49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m0.01\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   1801\u001b[39m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m   1803\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m nb_jobs == \u001b[32m0\u001b[39m:\n\u001b[32m   1804\u001b[39m     \u001b[38;5;66;03m# Case unordered: jobs are added to the list of jobs to\u001b[39;00m\n\u001b[32m   1805\u001b[39m     \u001b[38;5;66;03m# retrieve `self._jobs` only once completed or in error, which\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1811\u001b[39m     \u001b[38;5;66;03m# timeouts before any other dispatched job has completed and\u001b[39;00m\n\u001b[32m   1812\u001b[39m     \u001b[38;5;66;03m# been added to `self._jobs` to be retrieved.\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "for exp in refined_experiment_configs:\n",
    "    model_name = exp['model_name']\n",
    "    model_class = exp['model_class']\n",
    "    initial_params = exp['initial_params']\n",
    "    param_grid = exp['param_grid']\n",
    "    \n",
    "    print(f\"\\n--- Starting Time-Series Cross-Validation for {model_name} ---\")\n",
    "    \n",
    "    # Define TimeSeriesSplit with a fixed number of splits\n",
    "    tscv = TimeSeriesSplit(n_splits=5)\n",
    "    \n",
    "    cv_metrics = defaultdict(list)\n",
    "    \n",
    "    # Loop through each time-series split\n",
    "    for fold, (train_index, test_index) in enumerate(tscv.split(X_train_full)):\n",
    "        X_train, X_test = X_train_full.iloc[train_index], X_train_full.iloc[test_index]\n",
    "        y_train, y_test = y_train_full.iloc[train_index], y_train_full.iloc[test_index]\n",
    "        \n",
    "        print(f\"Fold {fold+1}/{tscv.n_splits}: Training on {len(X_train)} samples, testing on {len(X_test)} samples\")\n",
    "        \n",
    "        # Perform grid search for this specific fold\n",
    "        grid_search_fold = GridSearchCV(\n",
    "            estimator=model_class(**initial_params),\n",
    "            param_grid=param_grid,\n",
    "            scoring='f1_macro',\n",
    "            cv=TimeSeriesSplit(n_splits=2), # Use a nested TimeSeriesSplit for internal CV\n",
    "            n_jobs=-1\n",
    "        )\n",
    "        grid_search_fold.fit(X_train, y_train)\n",
    "        best_model_fold = grid_search_fold.best_estimator_\n",
    "        \n",
    "        # Evaluate the best model from this fold on the test split for this fold\n",
    "        y_pred = best_model_fold.predict(X_test)\n",
    "        \n",
    "        # Collect metrics for this fold\n",
    "        cv_metrics['accuracy'].append(accuracy_score(y_test, y_pred))\n",
    "        cv_metrics['precision'].append(precision_score(y_test, y_pred, zero_division=0))\n",
    "        cv_metrics['recall'].append(recall_score(y_test, y_pred, zero_division=0))\n",
    "        cv_metrics['f1'].append(f1_score(y_test, y_pred, zero_division=0))\n",
    "        cv_metrics['best_params'].append(grid_search_fold.best_params_)\n",
    "        \n",
    "    print(\"\\n--- Cross-Validation Results Summary ---\")\n",
    "    for metric, values in cv_metrics.items():\n",
    "        if metric == 'best_params':\n",
    "            # We don't average parameters, just print a list of the best found per fold\n",
    "            print(f\"Best params per fold: {values}\")\n",
    "        else:\n",
    "            print(f\"Average {metric}: {np.mean(values):.4f} (+/- {np.std(values):.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84dd4aff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Final Model Training on Full Training Data ---\n",
      "\n",
      "--- Final Model Evaluation on Unseen Test Data for LogisticRegression ---\n",
      "Best parameters found: {'C': np.float64(1.0), 'solver': 'liblinear'}\n",
      "Best cross-validation score: 0.39\n",
      "\n",
      "--- Final Classification Report ---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.16      0.26       250\n",
      "           1       0.53      0.98      0.69       243\n",
      "\n",
      "    accuracy                           0.56       493\n",
      "   macro avg       0.70      0.57      0.48       493\n",
      "weighted avg       0.70      0.56      0.47       493\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Final Model Training on Full Training Data ---\")\n",
    "    \n",
    "# Re-run a final GridSearchCV on the full training data to find the best parameters overall\n",
    "final_grid_search = GridSearchCV(\n",
    "    estimator=model_class(**initial_params),\n",
    "    param_grid=param_grid,\n",
    "    scoring='f1_macro',\n",
    "    cv=tscv, # Use TimeSeriesSplit for the final Grid Search\n",
    "    n_jobs=-1\n",
    ")\n",
    "final_grid_search.fit(X_train_full, y_train_full)\n",
    "best_model = final_grid_search.best_estimator_\n",
    "\n",
    "# Evaluate the final best model on the unseen test set\n",
    "y_pred_final = best_model.predict(X_test_full)\n",
    "\n",
    "report_str = classification_report(y_test_full, y_pred_final)\n",
    "\n",
    "print(f\"\\n--- Final Model Evaluation on Unseen Test Data for {model_name} ---\")\n",
    "print(f\"Best parameters found: {final_grid_search.best_params_}\")\n",
    "print(f\"Best cross-validation score: {final_grid_search.best_score_:.2f}\")\n",
    "print(\"\\n--- Final Classification Report ---\")\n",
    "print(report_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9795735c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Refined model saved as 'C:\\Users\\epoch_bpjmdqk\\Documents\\Code\\models\\logisticregression_w10_t0.005.pkl'\n"
     ]
    }
   ],
   "source": [
    "# Save model\n",
    "import pickle\n",
    "model_filename = f\"C:\\\\Users\\\\epoch_bpjmdqk\\\\Documents\\\\Code\\\\models\\\\{exp['model_name'].lower()}_w{window}_t{threshold}.pkl\"\n",
    "with open(model_filename, 'wb') as file:\n",
    "    pickle.dump(best_model, file)\n",
    "print(f\"\\n✅ Refined model saved as '{model_filename}'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

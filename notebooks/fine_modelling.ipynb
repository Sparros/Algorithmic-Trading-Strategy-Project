{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c249f71c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from sklearn.model_selection import GridSearchCV, TimeSeriesSplit\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, f1_score, precision_score, recall_score, accuracy_score\n",
    "from xgboost import XGBClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from collections import defaultdict\n",
    "\n",
    "# Add parent directory to path to access custom modules\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..')))\n",
    "from src.stock_features import create_target_variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e9801717",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Data setup for Window=10, Threshold=0.005 ---\n",
      "Class imbalance ratio (0/1): 0.96\n"
     ]
    }
   ],
   "source": [
    "# --- 1. DATA PREPARATION ---\n",
    "# Load the prepared data\n",
    "try:\n",
    "    data = pd.read_csv(r'C:\\Users\\epoch_bpjmdqk\\Documents\\Code\\data\\processed\\stock_and_macro.csv', index_col='Date', parse_dates=True)\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: The data file was not found. Please update the file path.\")\n",
    "    sys.exit()\n",
    "\n",
    "# Define the optimal data preparation parameters from the coarse modeling stage\n",
    "results = []\n",
    "window = 10\n",
    "threshold = 0.005\n",
    "target_ticker = 'WMT'\n",
    "split_date = '2021-01-01'\n",
    "\n",
    "# Create the target variable based on the selected parameters\n",
    "data_target = create_target_variable(data.copy(), target_ticker, window=window, threshold=threshold)\n",
    "\n",
    "# Define columns to drop to create the feature set (X)\n",
    "target_col_name = f'{target_ticker}_Target'\n",
    "target_return_col_name = f'{target_ticker}_target_return_{window}D_{threshold}'\n",
    "columns_to_drop = [\n",
    "    target_col_name,\n",
    "    target_return_col_name,\n",
    "    f'Open_{target_ticker}',\n",
    "    f'High_{target_ticker}',\n",
    "    f'Low_{target_ticker}',\n",
    "    f'Close_{target_ticker}'\n",
    "]\n",
    "\n",
    "# Handle NaN values and split the data\n",
    "data_target.dropna(inplace=True)\n",
    "X = data_target.drop(columns=columns_to_drop, errors='ignore')\n",
    "y = data_target[target_col_name]\n",
    "\n",
    "# Create a fixed training and testing set for the final evaluation\n",
    "# This is a fixed, chronological split, which is essential for financial data.\n",
    "X_train_full = X.loc[:split_date].copy()\n",
    "y_train_full = y.loc[:split_date].copy()\n",
    "X_test_full = X.loc[split_date:].copy()\n",
    "y_test_full = y.loc[split_date:].copy()\n",
    "\n",
    "neg_to_pos_ratio = (y_train_full == 0).sum() / (y_train_full == 1).sum()\n",
    "print(f\"\\n--- Data setup for Window={window}, Threshold={threshold} ---\")\n",
    "print(f\"Class imbalance ratio (0/1): {neg_to_pos_ratio:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5c15eb0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 2. DEFINE REFINED EXPERIMENT CONFIGURATIONS ---\n",
    "# We use the results from the coarse search to narrow down the hyperparameter space.\n",
    "refined_experiment_configs = [\n",
    "    # # Logistic Regression: Good baseline model.\n",
    "    # {\n",
    "    #     'model_name': 'LogisticRegression',\n",
    "    #     'model_class': LogisticRegression,\n",
    "    #     'initial_params': {'random_state': 42, 'class_weight': 'balanced'},\n",
    "    #     # Refined param grid based on coarse search results\n",
    "    #     'param_grid': {\n",
    "    #         'C': [0.05, 0.1, 0.5, 1.0],\n",
    "    #         'solver': ['liblinear']\n",
    "    #     }\n",
    "    # },\n",
    "    # # XGBoost: A powerful, gradient-boosting model often used for tabular data.\n",
    "    # {\n",
    "    #     'model_name': 'XGBoost',\n",
    "    #     'model_class': XGBClassifier,\n",
    "    #     'initial_params': {'eval_metric': 'logloss', 'use_label_encoder': False, 'random_state': 42, 'scale_pos_weight': neg_to_pos_ratio},\n",
    "    #     # Refined param grid based on coarse search results\n",
    "    #     'param_grid': {\n",
    "    #         'n_estimators': [75, 100, 125, 150],\n",
    "    #         'learning_rate': [0.08, 0.1, 0.12],\n",
    "    #         'max_depth': [3, 4],\n",
    "    #         'subsample': [0.8, 1.0],\n",
    "    #         'colsample_bytree': [0.8, 1.0]\n",
    "    #     }\n",
    "    # },\n",
    "    # # CatBoost: Another robust gradient-boosting model that handles categorical features automatically.\n",
    "    # {\n",
    "    #     'model_name': 'CatBoost',\n",
    "    #     'model_class': CatBoostClassifier,\n",
    "    #     'initial_params': {'verbose': False, 'random_state': 42, 'early_stopping_rounds': 50, 'class_weights': [1, neg_to_pos_ratio]},\n",
    "    #     # Refined param grid based on coarse search results\n",
    "    #     'param_grid': {\n",
    "    #         'n_estimators': [75, 100, 125, 150],\n",
    "    #         'learning_rate': [0.08, 0.1, 0.12],\n",
    "    #         'depth': [3, 4]\n",
    "    #     }\n",
    "    # },\n",
    "    # RandomForest: An ensemble tree method that is often less prone to overfitting.\n",
    "    {\n",
    "        'model_name': 'RandomForest',\n",
    "        'model_class': RandomForestClassifier,\n",
    "        'initial_params': {'random_state': 42, 'class_weight': 'balanced'},\n",
    "        # Refined param grid based on coarse search results\n",
    "        'param_grid': {\n",
    "            'n_estimators': [100, 150, 200, 250],\n",
    "            'max_depth': [4, 5, 6],\n",
    "            'min_samples_split': [2, 3],\n",
    "            'min_samples_leaf': [1, 2]\n",
    "        }\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7be48f4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Starting Time-Series Cross-Validation for RandomForest ---\n",
      "Fold 1/5: Training on 455 samples, testing on 451 samples\n",
      "Fold 2/5: Training on 906 samples, testing on 451 samples\n",
      "Fold 3/5: Training on 1357 samples, testing on 451 samples\n",
      "Fold 4/5: Training on 1808 samples, testing on 451 samples\n",
      "Fold 5/5: Training on 2259 samples, testing on 451 samples\n",
      "\n",
      "--- Cross-Validation Results Summary ---\n",
      "Average accuracy: 0.5162 (+/- 0.0600)\n",
      "Average precision: 0.4753 (+/- 0.2408)\n",
      "Average recall: 0.1817 (+/- 0.1800)\n",
      "Average f1: 0.2351 (+/- 0.1827)\n",
      "Best params per fold: [{'max_depth': 5, 'min_samples_leaf': 1, 'min_samples_split': 3, 'n_estimators': 100}, {'max_depth': 5, 'min_samples_leaf': 1, 'min_samples_split': 3, 'n_estimators': 100}, {'max_depth': 6, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 100}, {'max_depth': 5, 'min_samples_leaf': 2, 'min_samples_split': 2, 'n_estimators': 100}, {'max_depth': 5, 'min_samples_leaf': 1, 'min_samples_split': 3, 'n_estimators': 200}]\n"
     ]
    }
   ],
   "source": [
    "# --- 3. FINE-TUNING LOOP WITH TIME-SERIES CROSS-VALIDATION ---\n",
    "for exp in refined_experiment_configs:\n",
    "    model_name = exp['model_name']\n",
    "    model_class = exp['model_class']\n",
    "    initial_params = exp['initial_params']\n",
    "    param_grid = exp['param_grid']\n",
    "    \n",
    "    print(f\"\\n--- Starting Time-Series Cross-Validation for {model_name} ---\")\n",
    "    \n",
    "    # Define a TimeSeriesSplit object for the outer cross-validation loop.\n",
    "    # This will create chronological splits of the data.\n",
    "    tscv = TimeSeriesSplit(n_splits=5)\n",
    "    \n",
    "    cv_metrics = defaultdict(list)\n",
    "    \n",
    "    # Loop through each time-series split\n",
    "    # This is the outer cross-validation loop to get robust performance metrics\n",
    "    for fold, (train_index, test_index) in enumerate(tscv.split(X_train_full)):\n",
    "        X_train, X_test = X_train_full.iloc[train_index], X_train_full.iloc[test_index]\n",
    "        y_train, y_test = y_train_full.iloc[train_index], y_train_full.iloc[test_index]\n",
    "        \n",
    "        print(f\"Fold {fold+1}/{tscv.n_splits}: Training on {len(X_train)} samples, testing on {len(X_test)} samples\")\n",
    "        \n",
    "        # Perform grid search for this specific fold\n",
    "        # We use a nested TimeSeriesSplit to validate parameters within each fold.\n",
    "        grid_search_fold = GridSearchCV(\n",
    "            estimator=model_class(**initial_params),\n",
    "            param_grid=param_grid,\n",
    "            scoring='f1_macro', # Use F1 score for a balanced evaluation\n",
    "            cv=TimeSeriesSplit(n_splits=2), # Use a nested TimeSeriesSplit for internal CV\n",
    "            n_jobs=-1\n",
    "        )\n",
    "        grid_search_fold.fit(X_train, y_train)\n",
    "        best_model_fold = grid_search_fold.best_estimator_\n",
    "        \n",
    "        # Evaluate the best model from this fold on the held-out test split for this fold\n",
    "        y_pred = best_model_fold.predict(X_test)\n",
    "        \n",
    "        # Collect metrics for this fold\n",
    "        cv_metrics['accuracy'].append(accuracy_score(y_test, y_pred))\n",
    "        cv_metrics['precision'].append(precision_score(y_test, y_pred, zero_division=0))\n",
    "        cv_metrics['recall'].append(recall_score(y_test, y_pred, zero_division=0))\n",
    "        cv_metrics['f1'].append(f1_score(y_test, y_pred, zero_division=0))\n",
    "        cv_metrics['best_params'].append(grid_search_fold.best_params_)\n",
    "        \n",
    "    print(\"\\n--- Cross-Validation Results Summary ---\")\n",
    "    for metric, values in cv_metrics.items():\n",
    "        if metric == 'best_params':\n",
    "            # We don't average parameters, just print a list of the best found per fold\n",
    "            print(f\"Best params per fold: {values}\")\n",
    "        else:\n",
    "            print(f\"Average {metric}: {np.mean(values):.4f} (+/- {np.std(values):.4f})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5042f2e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Final Model Evaluation on Unseen Test Data for RandomForest ---\n",
      "Best parameters found: {'max_depth': 6, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 100}\n",
      "Best cross-validation score: 0.43\n",
      "\n",
      "--- Final Classification Report ---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      0.59      0.58       250\n",
      "           1       0.56      0.53      0.54       243\n",
      "\n",
      "    accuracy                           0.56       493\n",
      "   macro avg       0.56      0.56      0.56       493\n",
      "weighted avg       0.56      0.56      0.56       493\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# --- 4. FINAL MODEL TRAINING AND EVALUATION ---\n",
    "# Re-run a final GridSearchCV on the full training data to find the best parameters overall\n",
    "final_grid_search = GridSearchCV(\n",
    "    estimator=model_class(**initial_params),\n",
    "    param_grid=param_grid,\n",
    "    scoring='f1_macro',\n",
    "    cv=tscv, # Use TimeSeriesSplit for the final Grid Search\n",
    "    n_jobs=-1\n",
    ")\n",
    "final_grid_search.fit(X_train_full, y_train_full)\n",
    "best_model = final_grid_search.best_estimator_\n",
    "\n",
    "# Evaluate the final best model on the unseen test set\n",
    "y_pred_final = best_model.predict(X_test_full)\n",
    "report_str = classification_report(y_test_full, y_pred_final)\n",
    "\n",
    "print(f\"\\n--- Final Model Evaluation on Unseen Test Data for {model_name} ---\")\n",
    "print(f\"Best parameters found: {final_grid_search.best_params_}\")\n",
    "print(f\"Best cross-validation score: {final_grid_search.best_score_:.2f}\")\n",
    "print(\"\\n--- Final Classification Report ---\")\n",
    "print(report_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9795735c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Refined model saved as 'C:\\Users\\epoch_bpjmdqk\\Documents\\Code\\models\\randomforest_w10_t0.005.pkl'\n"
     ]
    }
   ],
   "source": [
    "# --- 5. SAVE THE FINAL MODEL ---\n",
    "# Ensure the models directory exists\n",
    "model_dir = \"C:\\\\Users\\\\epoch_bpjmdqk\\\\Documents\\\\Code\\\\models\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the best model using pickle\n",
    "model_filename = f\"{model_dir}\\\\{exp['model_name'].lower()}_w{window}_t{threshold}.pkl\"\n",
    "with open(model_filename, 'wb') as file:\n",
    "    pickle.dump(best_model, file)\n",
    "print(f\"\\n✅ Refined model saved as '{model_filename}'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

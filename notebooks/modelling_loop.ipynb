{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "0cb29ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..')))\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import GridSearchCV, TimeSeriesSplit\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "from src.stock_features import create_target_variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "1cfe2a70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the prepared data\n",
    "data = pd.read_csv(r'C:\\Users\\epoch_bpjmdqk\\Documents\\Code\\data\\processed\\stock_and_macro.csv', index_col='Date', parse_dates=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2f5614c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. DEFINE YOUR EXPERIMENTS ---\n",
    "experiment_configs = [\n",
    "    {\n",
    "        'model_name': 'XGBoost',\n",
    "        'model_class': XGBClassifier,\n",
    "        'initial_params': {'eval_metric': 'logloss', 'random_state': 42},\n",
    "        'param_grid': {\n",
    "            'n_estimators': [100], \n",
    "            'learning_rate': [0.1], \n",
    "            'max_depth': [3],\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        'model_name': 'CatBoost',\n",
    "        'model_class': CatBoostClassifier,\n",
    "        'initial_params': {'verbose': False, 'random_state': 42, 'early_stopping_rounds': 50},\n",
    "        'param_grid': {\n",
    "            'n_estimators': [100], \n",
    "            'learning_rate': [0.1], \n",
    "            'depth': [3],\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        'model_name': 'RandomForest',\n",
    "        'model_class': RandomForestClassifier,\n",
    "        'initial_params': {'random_state': 42, 'class_weight': 'balanced'},\n",
    "        'param_grid': {\n",
    "            'n_estimators': [100, 200], \n",
    "            'max_depth': [5, 10],\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        'model_name': 'LogisticRegression',\n",
    "        'model_class': LogisticRegression,\n",
    "        'initial_params': {'random_state': 42, 'class_weight': 'balanced'},\n",
    "        'param_grid': {\n",
    "            'C': [0.1, 1.0, 10.0], \n",
    "            'solver': ['liblinear']\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "# Define the different hyperparameters for the data preparation step\n",
    "window_sizes = [5, 10]\n",
    "thresholds = [0.005, 0.01]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29f60109",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Data setup for Window=5, Threshold=0.005 ---\n",
      "Class imbalance ratio (0/1): 1.21\n",
      "-> Running model: XGBoost\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\epoch_bpjmdqk\\Documents\\Code\\venv\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [16:56:39] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> Running model: CatBoost\n",
      "-> Running model: RandomForest\n",
      "-> Running model: LogisticRegression\n",
      "\n",
      "--- Data setup for Window=5, Threshold=0.01 ---\n",
      "Class imbalance ratio (0/1): 1.88\n",
      "-> Running model: XGBoost\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\epoch_bpjmdqk\\Documents\\Code\\venv\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [16:57:00] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> Running model: CatBoost\n",
      "-> Running model: RandomForest\n",
      "-> Running model: LogisticRegression\n",
      "\n",
      "--- Data setup for Window=10, Threshold=0.005 ---\n",
      "Class imbalance ratio (0/1): 0.97\n",
      "-> Running model: XGBoost\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\epoch_bpjmdqk\\Documents\\Code\\venv\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [16:57:13] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> Running model: CatBoost\n",
      "-> Running model: RandomForest\n",
      "-> Running model: LogisticRegression\n",
      "\n",
      "--- Data setup for Window=10, Threshold=0.01 ---\n",
      "Class imbalance ratio (0/1): 1.30\n",
      "-> Running model: XGBoost\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\epoch_bpjmdqk\\Documents\\Code\\venv\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [16:57:27] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> Running model: CatBoost\n",
      "-> Running model: RandomForest\n",
      "-> Running model: LogisticRegression\n",
      "\n",
      "--- Final Experiment Results Summary ---\n",
      "                 Model  Window  Threshold  Test_Accuracy  Test_Precision_1  Test_Recall_1  Test_F1_1                                                                                          Best_Params\n",
      "0   LogisticRegression      10      0.005       0.548708          0.517467       0.975309   0.676177                                                                    {'C': 0.1, 'solver': 'liblinear'}\n",
      "1   LogisticRegression      10      0.010       0.491054          0.454348       0.976636   0.620178                                                                    {'C': 1.0, 'solver': 'liblinear'}\n",
      "2   LogisticRegression       5      0.005       0.497018          0.465596       0.910314   0.616085                                                                    {'C': 1.0, 'solver': 'liblinear'}\n",
      "3              XGBoost      10      0.005       0.574553          0.560166       0.555556   0.557851  {'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 100, 'scale_pos_weight': 0.9681620839363242}\n",
      "4   LogisticRegression       5      0.010       0.415507          0.386555       0.989247   0.555891                                                                   {'C': 10.0, 'solver': 'liblinear'}\n",
      "5             CatBoost      10      0.010       0.560636          0.487085       0.616822   0.544330      {'depth': 3, 'learning_rate': 0.1, 'n_estimators': 100, 'scale_pos_weight': 1.3011844331641287}\n",
      "6         RandomForest      10      0.005       0.528827          0.512500       0.506173   0.509317                                                                {'max_depth': 5, 'n_estimators': 100}\n",
      "7         RandomForest      10      0.010       0.572565          0.497674       0.500000   0.498834                                                                {'max_depth': 5, 'n_estimators': 200}\n",
      "8              XGBoost      10      0.010       0.566600          0.490654       0.490654   0.490654  {'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 100, 'scale_pos_weight': 1.3011844331641287}\n",
      "9             CatBoost       5      0.005       0.528827          0.468468       0.466368   0.467416      {'depth': 3, 'learning_rate': 0.1, 'n_estimators': 100, 'scale_pos_weight': 1.2077922077922079}\n",
      "10            CatBoost       5      0.010       0.556660          0.417040       0.500000   0.454768      {'depth': 3, 'learning_rate': 0.1, 'n_estimators': 100, 'scale_pos_weight': 1.8783068783068784}\n",
      "11             XGBoost       5      0.005       0.544732          0.484694       0.426009   0.453461  {'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 100, 'scale_pos_weight': 1.2077922077922079}\n",
      "12        RandomForest       5      0.005       0.532803          0.470874       0.434978   0.452214                                                                {'max_depth': 5, 'n_estimators': 100}\n",
      "13        RandomForest       5      0.010       0.578529          0.435000       0.467742   0.450777                                                                {'max_depth': 5, 'n_estimators': 100}\n",
      "14             XGBoost       5      0.010       0.566600          0.423810       0.478495   0.449495  {'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 100, 'scale_pos_weight': 1.8783068783068784}\n",
      "15            CatBoost      10      0.005       0.526839          0.514451       0.366255   0.427885      {'depth': 3, 'learning_rate': 0.1, 'n_estimators': 100, 'scale_pos_weight': 0.9681620839363242}\n"
     ]
    }
   ],
   "source": [
    "# --- 2. THE EXPERIMENTAL LOOP (STAGE 1: MODEL SCREENING) ---\n",
    "results = []\n",
    "target_ticker = 'WMT'\n",
    "split_date = '2021-01-01'\n",
    "\n",
    "for window in window_sizes:\n",
    "    for threshold in thresholds:\n",
    "        # A. Dynamically create the target variable for this experiment\n",
    "        data_target = create_target_variable(data.copy(), target_ticker, window=window, threshold=threshold)\n",
    "\n",
    "        # B. Separate features (X) and target (y)\n",
    "        target_col_name = f'{target_ticker}_Target'\n",
    "        # Dynamically create the target return column name to match the function's output\n",
    "        target_return_col_name = f'{target_ticker}_target_return_{window}D_{threshold}'\n",
    "        \n",
    "        columns_to_drop = [\n",
    "            target_col_name,\n",
    "            target_return_col_name,\n",
    "            f'Open_{target_ticker}',\n",
    "            f'High_{target_ticker}',\n",
    "            f'Low_{target_ticker}',\n",
    "            f'Close_{target_ticker}'\n",
    "        ]\n",
    "\n",
    "        X = data_target.drop(columns=columns_to_drop)\n",
    "        y = data_target[target_col_name]\n",
    "        \n",
    "        X_train = X.loc[:split_date].copy()\n",
    "        y_train = y.loc[:split_date].copy()\n",
    "        X_test = X.loc[split_date:].copy()\n",
    "        y_test = y.loc[split_date:].copy()\n",
    "\n",
    "        # C. Handle Class Imbalance\n",
    "        neg_to_pos_ratio = (y_train == 0).sum() / (y_train == 1).sum()\n",
    "        print(f\"\\n--- Data setup for Window={window}, Threshold={threshold} ---\")\n",
    "        print(f\"Class imbalance ratio (0/1): {neg_to_pos_ratio:.2f}\")\n",
    "\n",
    "        for exp in experiment_configs:\n",
    "            print(f\"-> Running model: {exp['model_name']}\")\n",
    "            \n",
    "            # Create a new model instance for this run\n",
    "            model = exp['model_class'](**exp['initial_params'])\n",
    "            \n",
    "            # Get the param_grid and add the scale_pos_weight if necessary\n",
    "            param_grid = exp['param_grid'].copy()\n",
    "            if exp['model_name'] in ['XGBoost', 'CatBoost']:\n",
    "                param_grid['scale_pos_weight'] = [neg_to_pos_ratio]\n",
    "\n",
    "            # D. Fit with a lightweight GridSearchCV\n",
    "            grid_search = GridSearchCV(\n",
    "                estimator=model,\n",
    "                param_grid=param_grid,\n",
    "                scoring='f1_macro',\n",
    "                cv=3,\n",
    "                verbose=0,\n",
    "                n_jobs=-1\n",
    "            )\n",
    "            \n",
    "            grid_search.fit(X_train, y_train)\n",
    "            best_model = grid_search.best_estimator_\n",
    "\n",
    "            # E. Evaluate the model and collect results\n",
    "            y_pred = best_model.predict(X_test)\n",
    "            report = classification_report(y_test, y_pred, output_dict=True)\n",
    "            \n",
    "            # F. Extract and store top features\n",
    "            top_features = {}\n",
    "            if hasattr(best_model, 'feature_importances_'):\n",
    "                importances = pd.Series(best_model.feature_importances_, index=X_train.columns)\n",
    "                top_features = importances.nlargest(10).to_dict()\n",
    "            elif hasattr(best_model, 'coef_'):\n",
    "                coefficients = pd.Series(best_model.coef_[0], index=X_train.columns)\n",
    "                top_features = coefficients.abs().nlargest(10).to_dict()\n",
    "            \n",
    "            print(\"   Top 10 Features (or Coefficients):\")\n",
    "            for feature, score in top_features.items():\n",
    "                print(f\"   - {feature}: {score:.4f}\")\n",
    "            \n",
    "            results.append({\n",
    "                'Model': exp['model_name'],\n",
    "                'Window': window,\n",
    "                'Threshold': threshold,\n",
    "                'Test_Accuracy': report['accuracy'],\n",
    "                'Test_Precision_1': report['1']['precision'],\n",
    "                'Test_Recall_1': report['1']['recall'],\n",
    "                'Test_F1_1': report['1']['f1-score'],\n",
    "                'Best_Params': grid_search.best_params_,\n",
    "                'Top_Features': top_features\n",
    "            })\n",
    "\n",
    "# --- 3. DISPLAY FINAL RESULTS ---\n",
    "results_df = pd.DataFrame(results)\n",
    "# Sort the results by F1 score in descending order and reset the index\n",
    "results_df = results_df.sort_values(by='Test_F1_1', ascending=False).reset_index(drop=True)\n",
    "print(\"\\n--- Final Experiment Results Summary ---\")\n",
    "print(results_df.to_string())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "315e169e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Final Experiment Results Summary ---\n",
      "                 Model  Window  Threshold  Test_Accuracy  Test_Precision_1  Test_Recall_1  Test_F1_1                                                                                          Best_Params\n",
      "0   LogisticRegression      10      0.005       0.548708          0.517467       0.975309   0.676177                                                                    {'C': 0.1, 'solver': 'liblinear'}\n",
      "1   LogisticRegression      10      0.010       0.491054          0.454348       0.976636   0.620178                                                                    {'C': 1.0, 'solver': 'liblinear'}\n",
      "2   LogisticRegression       5      0.005       0.497018          0.465596       0.910314   0.616085                                                                    {'C': 1.0, 'solver': 'liblinear'}\n",
      "3              XGBoost      10      0.005       0.574553          0.560166       0.555556   0.557851  {'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 100, 'scale_pos_weight': 0.9681620839363242}\n",
      "4   LogisticRegression       5      0.010       0.415507          0.386555       0.989247   0.555891                                                                   {'C': 10.0, 'solver': 'liblinear'}\n",
      "5             CatBoost      10      0.010       0.560636          0.487085       0.616822   0.544330      {'depth': 3, 'learning_rate': 0.1, 'n_estimators': 100, 'scale_pos_weight': 1.3011844331641287}\n",
      "6         RandomForest      10      0.005       0.528827          0.512500       0.506173   0.509317                                                                {'max_depth': 5, 'n_estimators': 100}\n",
      "7         RandomForest      10      0.010       0.572565          0.497674       0.500000   0.498834                                                                {'max_depth': 5, 'n_estimators': 200}\n",
      "8              XGBoost      10      0.010       0.566600          0.490654       0.490654   0.490654  {'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 100, 'scale_pos_weight': 1.3011844331641287}\n",
      "9             CatBoost       5      0.005       0.528827          0.468468       0.466368   0.467416      {'depth': 3, 'learning_rate': 0.1, 'n_estimators': 100, 'scale_pos_weight': 1.2077922077922079}\n",
      "10            CatBoost       5      0.010       0.556660          0.417040       0.500000   0.454768      {'depth': 3, 'learning_rate': 0.1, 'n_estimators': 100, 'scale_pos_weight': 1.8783068783068784}\n",
      "11             XGBoost       5      0.005       0.544732          0.484694       0.426009   0.453461  {'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 100, 'scale_pos_weight': 1.2077922077922079}\n",
      "12        RandomForest       5      0.005       0.532803          0.470874       0.434978   0.452214                                                                {'max_depth': 5, 'n_estimators': 100}\n",
      "13        RandomForest       5      0.010       0.578529          0.435000       0.467742   0.450777                                                                {'max_depth': 5, 'n_estimators': 100}\n",
      "14             XGBoost       5      0.010       0.566600          0.423810       0.478495   0.449495  {'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 100, 'scale_pos_weight': 1.8783068783068784}\n",
      "15            CatBoost      10      0.005       0.526839          0.514451       0.366255   0.427885      {'depth': 3, 'learning_rate': 0.1, 'n_estimators': 100, 'scale_pos_weight': 0.9681620839363242}\n"
     ]
    }
   ],
   "source": [
    "# Sort the results by F1 score in descending order and reset the index\n",
    "results_df = results_df.sort_values(by='Test_F1_1', ascending=False).reset_index(drop=True)\n",
    "print(\"\\n--- Final Experiment Results Summary ---\")\n",
    "print(results_df.to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a4a918dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- All Data Columns ---\n",
      "['Close_COST', 'Close_KO', 'Close_PEP', 'Close_PG', 'Close_WMT', 'Close_^GSPC', 'High_COST', 'High_KO', 'High_PEP', 'High_PG', 'High_WMT', 'High_^GSPC', 'Low_COST', 'Low_KO', 'Low_PEP', 'Low_PG', 'Low_WMT', 'Low_^GSPC', 'Open_COST', 'Open_KO', 'Open_PEP', 'Open_PG', 'Open_WMT', 'Open_^GSPC', 'Volume_COST', 'Volume_KO', 'Volume_PEP', 'Volume_PG', 'Volume_WMT', 'Volume_^GSPC', 'COST_HighLow_Range', 'COST_OpenClose_Range', 'COST_Close_to_Range_Ratio', 'COST_True_Range', 'COST_ATR14', 'COST_Volume_Daily_Change', 'COST_Volume_MA_20D', 'COST_Volume_MA_Ratio', 'COST_OBV', 'COST_RSI14', 'COST_MACD_Line', 'COST_MACD_Signal', 'COST_MACD_Hist', 'COST_SMA_10', 'COST_SMA_20', 'COST_SMA_50', 'COST_EMA_12', 'COST_EMA_26', 'COST_BB_Middle20', 'COST_BB_Upper20', 'COST_BB_Lower20', 'COST_BB_Bandwidth20', 'COST_BB_PctB20', 'COST_Stoch_K_14', 'COST_Stoch_D_14_3', 'COST_PlusDI_14', 'COST_MinusDI_14', 'COST_DX_14', 'COST_ADX_14', 'KO_HighLow_Range', 'KO_OpenClose_Range', 'KO_Close_to_Range_Ratio', 'KO_True_Range', 'KO_ATR14', 'KO_Volume_Daily_Change', 'KO_Volume_MA_20D', 'KO_Volume_MA_Ratio', 'KO_OBV', 'KO_RSI14', 'KO_MACD_Line', 'KO_MACD_Signal', 'KO_MACD_Hist', 'KO_SMA_10', 'KO_SMA_20', 'KO_SMA_50', 'KO_EMA_12', 'KO_EMA_26', 'KO_BB_Middle20', 'KO_BB_Upper20', 'KO_BB_Lower20', 'KO_BB_Bandwidth20', 'KO_BB_PctB20', 'KO_Stoch_K_14', 'KO_Stoch_D_14_3', 'KO_PlusDI_14', 'KO_MinusDI_14', 'KO_DX_14', 'KO_ADX_14', 'PEP_HighLow_Range', 'PEP_OpenClose_Range', 'PEP_Close_to_Range_Ratio', 'PEP_True_Range', 'PEP_ATR14', 'PEP_Volume_Daily_Change', 'PEP_Volume_MA_20D', 'PEP_Volume_MA_Ratio', 'PEP_OBV', 'PEP_RSI14', 'PEP_MACD_Line', 'PEP_MACD_Signal', 'PEP_MACD_Hist', 'PEP_SMA_10', 'PEP_SMA_20', 'PEP_SMA_50', 'PEP_EMA_12', 'PEP_EMA_26', 'PEP_BB_Middle20', 'PEP_BB_Upper20', 'PEP_BB_Lower20', 'PEP_BB_Bandwidth20', 'PEP_BB_PctB20', 'PEP_Stoch_K_14', 'PEP_Stoch_D_14_3', 'PEP_PlusDI_14', 'PEP_MinusDI_14', 'PEP_DX_14', 'PEP_ADX_14', 'PG_HighLow_Range', 'PG_OpenClose_Range', 'PG_Close_to_Range_Ratio', 'PG_True_Range', 'PG_ATR14', 'PG_Volume_Daily_Change', 'PG_Volume_MA_20D', 'PG_Volume_MA_Ratio', 'PG_OBV', 'PG_RSI14', 'PG_MACD_Line', 'PG_MACD_Signal', 'PG_MACD_Hist', 'PG_SMA_10', 'PG_SMA_20', 'PG_SMA_50', 'PG_EMA_12', 'PG_EMA_26', 'PG_BB_Middle20', 'PG_BB_Upper20', 'PG_BB_Lower20', 'PG_BB_Bandwidth20', 'PG_BB_PctB20', 'PG_Stoch_K_14', 'PG_Stoch_D_14_3', 'PG_PlusDI_14', 'PG_MinusDI_14', 'PG_DX_14', 'PG_ADX_14', 'WMT_HighLow_Range', 'WMT_OpenClose_Range', 'WMT_Close_to_Range_Ratio', 'WMT_True_Range', 'WMT_ATR14', 'WMT_Volume_Daily_Change', 'WMT_Volume_MA_20D', 'WMT_Volume_MA_Ratio', 'WMT_OBV', 'WMT_RSI14', 'WMT_MACD_Line', 'WMT_MACD_Signal', 'WMT_MACD_Hist', 'WMT_SMA_10', 'WMT_SMA_20', 'WMT_SMA_50', 'WMT_EMA_12', 'WMT_EMA_26', 'WMT_BB_Middle20', 'WMT_BB_Upper20', 'WMT_BB_Lower20', 'WMT_BB_Bandwidth20', 'WMT_BB_PctB20', 'WMT_Stoch_K_14', 'WMT_Stoch_D_14_3', 'WMT_PlusDI_14', 'WMT_MinusDI_14', 'WMT_DX_14', 'WMT_ADX_14', '^GSPC_HighLow_Range', '^GSPC_OpenClose_Range', '^GSPC_Close_to_Range_Ratio', '^GSPC_True_Range', '^GSPC_ATR14', '^GSPC_Volume_Daily_Change', '^GSPC_Volume_MA_20D', '^GSPC_Volume_MA_Ratio', '^GSPC_OBV', '^GSPC_RSI14', '^GSPC_MACD_Line', '^GSPC_MACD_Signal', '^GSPC_MACD_Hist', '^GSPC_SMA_10', '^GSPC_SMA_20', '^GSPC_SMA_50', '^GSPC_EMA_12', '^GSPC_EMA_26', '^GSPC_BB_Middle20', '^GSPC_BB_Upper20', '^GSPC_BB_Lower20', '^GSPC_BB_Bandwidth20', '^GSPC_BB_PctB20', '^GSPC_Stoch_K_14', '^GSPC_Stoch_D_14_3', '^GSPC_PlusDI_14', '^GSPC_MinusDI_14', '^GSPC_DX_14', '^GSPC_ADX_14', 'Close_COST_daily_return', 'Close_KO_daily_return', 'Close_PEP_daily_return', 'Close_PG_daily_return', 'Close_WMT_daily_return', 'Close_^GSPC_daily_return', 'WMT_vs_^GSPC_RelStrength', 'WMT_vs_^GSPC_ATR_Ratio', 'WMT_vs_^GSPC_VolumeRatio', 'WMT_vs_KO_CloseRatio', 'WMT_vs_PEP_CloseRatio', 'WMT_Volume_x_ATR', 'WMT_vs_KO_RollingCorr_5D', 'WMT_vs_PEP_RollingCorr_5D', 'Close_COST_lag1', 'Close_COST_lag3', 'Close_COST_lag5', 'Close_COST_daily_return_lag1', 'Close_COST_daily_return_lag3', 'Close_COST_daily_return_lag5', 'COST_RSI14_lag1', 'COST_RSI14_lag3', 'COST_RSI14_lag5', 'COST_Volume_MA_Ratio_lag1', 'COST_Volume_MA_Ratio_lag3', 'COST_Volume_MA_Ratio_lag5', 'Close_KO_lag1', 'Close_KO_lag3', 'Close_KO_lag5', 'Close_KO_daily_return_lag1', 'Close_KO_daily_return_lag3', 'Close_KO_daily_return_lag5', 'KO_RSI14_lag1', 'KO_RSI14_lag3', 'KO_RSI14_lag5', 'KO_Volume_MA_Ratio_lag1', 'KO_Volume_MA_Ratio_lag3', 'KO_Volume_MA_Ratio_lag5', 'Close_PEP_lag1', 'Close_PEP_lag3', 'Close_PEP_lag5', 'Close_PEP_daily_return_lag1', 'Close_PEP_daily_return_lag3', 'Close_PEP_daily_return_lag5', 'PEP_RSI14_lag1', 'PEP_RSI14_lag3', 'PEP_RSI14_lag5', 'PEP_Volume_MA_Ratio_lag1', 'PEP_Volume_MA_Ratio_lag3', 'PEP_Volume_MA_Ratio_lag5', 'Close_PG_lag1', 'Close_PG_lag3', 'Close_PG_lag5', 'Close_PG_daily_return_lag1', 'Close_PG_daily_return_lag3', 'Close_PG_daily_return_lag5', 'PG_RSI14_lag1', 'PG_RSI14_lag3', 'PG_RSI14_lag5', 'PG_Volume_MA_Ratio_lag1', 'PG_Volume_MA_Ratio_lag3', 'PG_Volume_MA_Ratio_lag5', 'Close_WMT_lag1', 'Close_WMT_lag3', 'Close_WMT_lag5', 'Close_WMT_daily_return_lag1', 'Close_WMT_daily_return_lag3', 'Close_WMT_daily_return_lag5', 'WMT_RSI14_lag1', 'WMT_RSI14_lag3', 'WMT_RSI14_lag5', 'WMT_Volume_MA_Ratio_lag1', 'WMT_Volume_MA_Ratio_lag3', 'WMT_Volume_MA_Ratio_lag5', 'Close_^GSPC_lag1', 'Close_^GSPC_lag3', 'Close_^GSPC_lag5', 'Close_^GSPC_daily_return_lag1', 'Close_^GSPC_daily_return_lag3', 'Close_^GSPC_daily_return_lag5', '^GSPC_RSI14_lag1', '^GSPC_RSI14_lag3', '^GSPC_RSI14_lag5', '^GSPC_Volume_MA_Ratio_lag1', '^GSPC_Volume_MA_Ratio_lag3', '^GSPC_Volume_MA_Ratio_lag5', 'PAYEMS', 'UNRATE', 'DFF', 'CPIAUCSL', 'RSAFS', 'GDPC1', 'DGS10']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#print all data coloums\n",
    "print(\"\\n--- All Data Columns ---\")\n",
    "print(data.columns.tolist())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

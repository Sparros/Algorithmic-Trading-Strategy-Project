{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cb29ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import GridSearchCV, TimeSeriesSplit\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "# Add parent directory to path to access custom modules\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..')))\n",
    "from src.stock_features import create_target_variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cfe2a70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. DATA PREPARATION ---\n",
    "try:\n",
    "    data = pd.read_csv(r'C:\\Users\\epoch_bpjmdqk\\Documents\\Code\\data\\processed\\stock_and_macro_filtered.csv', index_col='Date', parse_dates=True)\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: The data file was not found. Please update the file path to point to your processed data.\")\n",
    "    sys.exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2f5614c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 2. DEFINE THE EXPERIMENTS ---\n",
    "# A list of dictionaries, where each dictionary defines an experiment with a specific model and its parameters.\n",
    "experiment_configs = [\n",
    "    {\n",
    "        'model_name': 'XGBoost',\n",
    "        'model_class': XGBClassifier,\n",
    "        'initial_params': {'eval_metric': 'logloss', 'random_state': 42},\n",
    "        'param_grid': {\n",
    "            'n_estimators': [100], \n",
    "            'learning_rate': [0.1], \n",
    "            'max_depth': [3],\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        'model_name': 'CatBoost',\n",
    "        'model_class': CatBoostClassifier,\n",
    "        'initial_params': {'verbose': False, 'random_state': 42, 'early_stopping_rounds': 50},\n",
    "        'param_grid': {\n",
    "            'n_estimators': [100], \n",
    "            'learning_rate': [0.1], \n",
    "            'depth': [3],\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        'model_name': 'RandomForest',\n",
    "        'model_class': RandomForestClassifier,\n",
    "        'initial_params': {'random_state': 42, 'class_weight': 'balanced'},\n",
    "        'param_grid': {\n",
    "            'n_estimators': [100, 200], \n",
    "            'max_depth': [5, 10],\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        'model_name': 'LogisticRegression',\n",
    "        'model_class': LogisticRegression,\n",
    "        'initial_params': {'random_state': 42, 'class_weight': 'balanced'},\n",
    "        'param_grid': {\n",
    "            'C': [0.1, 1.0, 10.0], \n",
    "            'solver': ['liblinear']\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "# Define the different hyperparameters for the data preparation step\n",
    "window_sizes = [5, 10]\n",
    "thresholds = [0.005, 0.01]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29f60109",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 3. THE EXPERIMENTAL LOOP (MODEL SCREENING) ---\n",
    "results = []\n",
    "target_ticker = 'WMT'\n",
    "split_date = '2021-01-01'\n",
    "\n",
    "for window in window_sizes:\n",
    "    for threshold in thresholds:\n",
    "        print(f\"\\n--- Starting Model Screening for Window={window}, Threshold={threshold} ---\")\n",
    "\n",
    "        # A. Dynamically create the target variable for this experiment\n",
    "        data_target = create_target_variable(data.copy(), target_ticker, window=window, threshold=threshold)\n",
    "\n",
    "        # B. Define features (X) and target (y)\n",
    "        # Dynamically get the target column name from the `create_target_variable` function output\n",
    "        target_col_name = f'{target_ticker}_Target'\n",
    "        target_return_col_name = f'{target_ticker}_target_return_{window}D_{threshold}'\n",
    "        \n",
    "        # Define columns to drop to create the feature set (X)\n",
    "        columns_to_drop = [\n",
    "            target_col_name,\n",
    "            target_return_col_name,\n",
    "            f'Open_{target_ticker}',\n",
    "            f'High_{target_ticker}',\n",
    "            f'Low_{target_ticker}',\n",
    "            f'Close_{target_ticker}'\n",
    "        ]\n",
    "\n",
    "        # Create feature (X) and target (y) sets\n",
    "        X = data_target.drop(columns=columns_to_drop, errors='ignore')\n",
    "        y = data_target[target_col_name]\n",
    "        \n",
    "        # Split data into training and testing sets based on the split date\n",
    "        X_train = X.loc[:split_date].copy()\n",
    "        y_train = y.loc[:split_date].copy()\n",
    "        X_test = X.loc[split_date:].copy()\n",
    "        y_test = y.loc[split_date:].copy()\n",
    "\n",
    "        # C. Handle Class Imbalance\n",
    "        neg_to_pos_ratio = (y_train == 0).sum() / (y_train == 1).sum()\n",
    "        print(f\"Class imbalance ratio (0/1): {neg_to_pos_ratio:.2f}\")\n",
    "\n",
    "        for exp in experiment_configs:\n",
    "            print(f\"-> Running model: {exp['model_name']}\")\n",
    "            \n",
    "            # Create a new model instance for this run\n",
    "            model = exp['model_class'](**exp['initial_params'])\n",
    "            \n",
    "            # Get the param_grid and add the scale_pos_weight for tree-based models\n",
    "            param_grid = exp['param_grid'].copy()\n",
    "            if exp['model_name'] in ['XGBoost', 'CatBoost']:\n",
    "                param_grid['scale_pos_weight'] = [neg_to_pos_ratio]\n",
    "            # Logistic Regression and RandomForest have `class_weight='balanced'` which handles this internally\n",
    "\n",
    "            # D. Fit with a lightweight GridSearchCV\n",
    "            # Uses a TimeSeriesSplit-like cross-validation (cv=3) for initial tuning\n",
    "            grid_search = GridSearchCV(\n",
    "                estimator=model,\n",
    "                param_grid=param_grid,\n",
    "                scoring='f1_macro', # Use F1 score for a balanced evaluation on imbalanced data\n",
    "                cv=3,\n",
    "                verbose=0,\n",
    "                n_jobs=-1\n",
    "            )\n",
    "            \n",
    "            grid_search.fit(X_train, y_train)\n",
    "            best_model = grid_search.best_estimator_\n",
    "\n",
    "            # E. Evaluate the model and collect results\n",
    "            y_pred = best_model.predict(X_test)\n",
    "            report = classification_report(y_test, y_pred, output_dict=True)\n",
    "            \n",
    "            # F. Extract and store top features\n",
    "            top_features = {}\n",
    "            if hasattr(best_model, 'feature_importances_'):\n",
    "                # For tree-based models like Random Forest, XGBoost, CatBoost\n",
    "                importances = pd.Series(best_model.feature_importances_, index=X_train.columns)\n",
    "                top_features = importances.nlargest(10).to_dict()\n",
    "            elif hasattr(best_model, 'coef_'):\n",
    "                # For linear models like Logistic Regression\n",
    "                coefficients = pd.Series(best_model.coef_[0], index=X_train.columns)\n",
    "                top_features = coefficients.abs().nlargest(10).to_dict()\n",
    "            \n",
    "            print(\"   Top 10 Features (or Coefficients):\")\n",
    "            for feature, score in top_features.items():\n",
    "                print(f\"   - {feature}: {score:.4f}\")\n",
    "            \n",
    "            results.append({\n",
    "                'Model': exp['model_name'],\n",
    "                'Window': window,\n",
    "                'Threshold': threshold,\n",
    "                'Test_Accuracy': report['accuracy'],\n",
    "                'Test_Precision_1': report['1']['precision'],\n",
    "                'Test_Recall_1': report['1']['recall'],\n",
    "                'Test_F1_1': report['1']['f1-score'],\n",
    "                'Best_Params': grid_search.best_params_,\n",
    "                'Top_Features': top_features\n",
    "            })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c0891bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 4. DISPLAY FINAL RESULTS ---\n",
    "# Convert results list to a DataFrame for easy viewing\n",
    "results_df = pd.DataFrame(results)\n",
    "# Sort the results by F1 score in descending order and reset the index\n",
    "results_df = results_df.sort_values(by='Test_F1_1', ascending=False).reset_index(drop=True)\n",
    "results_df.drop(columns=['Top_Features'], inplace=True) # Drop top features for cleaner output\n",
    "\n",
    "print(\"\\n--- Final Experiment Results Summary ---\")\n",
    "print(results_df.to_string())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

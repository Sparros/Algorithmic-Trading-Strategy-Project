{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cb29ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import GridSearchCV, TimeSeriesSplit\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Add parent directory to path to access custom modules\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..')))\n",
    "from src.stock_features import create_target_variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1cfe2a70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. DATA PREPARATION ---\n",
    "try:\n",
    "    data = pd.read_csv(r'C:\\Users\\epoch_bpjmdqk\\Documents\\Code\\data\\processed\\consumer_staples_data.csv', index_col='Date', parse_dates=True)\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: The data file was not found. Please update the file path to point to your processed data.\")\n",
    "    sys.exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c2f5614c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 2. DEFINE THE EXPERIMENTS ---\n",
    "# A list of dictionaries, where each dictionary defines an experiment with a specific model and its parameters.\n",
    "experiment_configs = [\n",
    "    {\n",
    "        'model_name': 'XGBoost',\n",
    "        'model_class': XGBClassifier,\n",
    "        'initial_params': {'eval_metric': 'logloss', 'random_state': 42},\n",
    "        'param_grid': {\n",
    "            'n_estimators': [100], \n",
    "            'learning_rate': [0.1], \n",
    "            'max_depth': [3],\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        'model_name': 'CatBoost',\n",
    "        'model_class': CatBoostClassifier,\n",
    "        'initial_params': {'verbose': False, 'random_state': 42, 'early_stopping_rounds': 50},\n",
    "        'param_grid': {\n",
    "            'n_estimators': [100], \n",
    "            'learning_rate': [0.1], \n",
    "            'depth': [3],\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        'model_name': 'RandomForest',\n",
    "        'model_class': RandomForestClassifier,\n",
    "        'initial_params': {'random_state': 42, 'class_weight': 'balanced'},\n",
    "        'param_grid': {\n",
    "            'n_estimators': [100, 200], \n",
    "            'max_depth': [5, 10],\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        'model_name': 'LogisticRegression',\n",
    "        'model_class': LogisticRegression,\n",
    "        'initial_params': {'random_state': 42, 'class_weight': 'balanced'},\n",
    "        'param_grid': {\n",
    "            'C': [0.1, 1.0, 10.0], \n",
    "            'solver': ['liblinear']\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "# Define the different hyperparameters for the data preparation step\n",
    "window_sizes = [5, 10]\n",
    "thresholds = [0.005, 0.01]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "29f60109",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Starting Model Screening for Window=5, Threshold=0.005 ---\n",
      "Class imbalance ratio (0/1): 1.15\n",
      "-> Running model: XGBoost\n",
      "   Top 10 Features (or Coefficients):\n",
      "   - ^GSPC_BB_PctB20: 0.0220\n",
      "   - Close_^GSPC: 0.0171\n",
      "   - Kalman_Filtered_Close_COST_lag_5: 0.0160\n",
      "   - WMT_vs_^GSPC_ATR_Ratio: 0.0131\n",
      "   - Kalman_Filtered_Close_COST_lag_10: 0.0121\n",
      "   - PEP_Stoch_D_14_3: 0.0116\n",
      "   - ^GSPC_Stoch_K_14: 0.0115\n",
      "   - ^GSPC_RSI14: 0.0111\n",
      "   - WMT_ATR14: 0.0111\n",
      "   - COST_SMA_50: 0.0104\n",
      "-> Running model: CatBoost\n",
      "   Top 10 Features (or Coefficients):\n",
      "   - WMT_vs_^GSPC_ATR_Ratio: 4.1821\n",
      "   - WMT_vs_KO_CloseRatio: 3.1833\n",
      "   - WMT_SMA_20: 2.9111\n",
      "   - WMT_vs_PEP_CloseRatio: 2.5939\n",
      "   - PG_CMF_21: 2.4695\n",
      "   - DGS10: 2.4219\n",
      "   - ^GSPC_MACD_Line: 2.0448\n",
      "   - ^GSPC_RSI14: 1.8881\n",
      "   - PG_MACD_Signal: 1.8044\n",
      "   - ^GSPC_MACD_Signal: 1.7458\n",
      "-> Running model: RandomForest\n",
      "   Top 10 Features (or Coefficients):\n",
      "   - WMT_vs_KO_CloseRatio: 0.0198\n",
      "   - ^GSPC_RollingMean_Convergence_50: 0.0179\n",
      "   - WMT_vs_PEP_CloseRatio: 0.0168\n",
      "   - WMT_vs_^GSPC_ATR_Ratio: 0.0153\n",
      "   - ^GSPC_MinusDI_14: 0.0148\n",
      "   - ^GSPC_CMF_21: 0.0121\n",
      "   - ^GSPC_Volume_MA_20D: 0.0114\n",
      "   - WMT_BB_Lower20: 0.0112\n",
      "   - WMT_EMA_12: 0.0110\n",
      "   - Kalman_Filtered_Close_WMT_lag_5: 0.0107\n",
      "-> Running model: LogisticRegression\n",
      "   Top 10 Features (or Coefficients):\n",
      "   - COST_TrueRange_Vol_Interaction: 0.0000\n",
      "   - WMT_Volume_MA_20D: 0.0000\n",
      "   - WMT_TrueRange_Vol_Interaction: 0.0000\n",
      "   - KO_Volume_MA_20D: 0.0000\n",
      "   - Volume_COST: 0.0000\n",
      "   - PG_TrueRange_Vol_Interaction: 0.0000\n",
      "   - PG_Volume_MA_20D: 0.0000\n",
      "   - PEP_Volume_MA_20D: 0.0000\n",
      "   - Volume_PG: 0.0000\n",
      "   - Volume_KO: 0.0000\n",
      "\n",
      "--- Starting Model Screening for Window=5, Threshold=0.01 ---\n",
      "Class imbalance ratio (0/1): 1.57\n",
      "-> Running model: XGBoost\n",
      "   Top 10 Features (or Coefficients):\n",
      "   - RSAFS: 0.0179\n",
      "   - CPIAUCSL: 0.0173\n",
      "   - ^GSPC_Stoch_D_14_3: 0.0158\n",
      "   - WMT_BB_Lower20: 0.0153\n",
      "   - PEP_SMA_10: 0.0150\n",
      "   - WMT_EMA_12: 0.0144\n",
      "   - Close_^GSPC: 0.0134\n",
      "   - WMT_SMA_50: 0.0128\n",
      "   - WMT_vs_^GSPC_ATR_Ratio: 0.0125\n",
      "   - High_^GSPC: 0.0124\n",
      "-> Running model: CatBoost\n",
      "   Top 10 Features (or Coefficients):\n",
      "   - PG_CMF_21: 3.4043\n",
      "   - WMT_vs_^GSPC_ATR_Ratio: 3.2175\n",
      "   - PG_MACD_Signal: 2.9113\n",
      "   - WMT_RollingMean_Convergence_50: 2.6440\n",
      "   - WMT_BB_Bandwidth20: 2.2136\n",
      "   - ^GSPC_BB_PctB20: 2.1745\n",
      "   - ^GSPC_CMF_21: 2.0979\n",
      "   - WMT_vs_PEP_CloseRatio: 2.0731\n",
      "   - PG_MACD_Line: 2.0696\n",
      "   - KO_BB_Bandwidth20: 2.0474\n",
      "-> Running model: RandomForest\n",
      "   Top 10 Features (or Coefficients):\n",
      "   - ^GSPC_RollingMean_Convergence_50: 0.0203\n",
      "   - WMT_EMA_12: 0.0168\n",
      "   - WMT_BB_Middle20: 0.0152\n",
      "   - Kalman_Filtered_Close_WMT_lag_1: 0.0151\n",
      "   - ^GSPC_BB_PctB20: 0.0150\n",
      "   - WMT_BB_Lower20: 0.0149\n",
      "   - WMT_vs_^GSPC_ATR_Ratio: 0.0141\n",
      "   - ^GSPC_Stoch_K_14: 0.0139\n",
      "   - WMT_EMA_26: 0.0128\n",
      "   - ^GSPC_MACD_Line: 0.0117\n",
      "-> Running model: LogisticRegression\n",
      "   Top 10 Features (or Coefficients):\n",
      "   - WMT_TrueRange_Vol_Interaction: 0.0000\n",
      "   - COST_OBV: 0.0000\n",
      "   - KO_Volume_MA_20D: 0.0000\n",
      "   - PG_TrueRange_Vol_Interaction: 0.0000\n",
      "   - COST_TrueRange_Vol_Interaction: 0.0000\n",
      "   - WMT_Volume_MA_20D: 0.0000\n",
      "   - Volume_COST: 0.0000\n",
      "   - Volume_WMT: 0.0000\n",
      "   - PG_Volume_MA_20D: 0.0000\n",
      "   - PEP_Volume_MA_20D: 0.0000\n",
      "\n",
      "--- Starting Model Screening for Window=10, Threshold=0.005 ---\n",
      "Class imbalance ratio (0/1): 0.96\n",
      "-> Running model: XGBoost\n",
      "   Top 10 Features (or Coefficients):\n",
      "   - ^GSPC_RollingMean_Convergence_50: 0.0182\n",
      "   - Kalman_Filtered_Close_COST_lag_10: 0.0174\n",
      "   - ^GSPC_MFI_14: 0.0143\n",
      "   - ^GSPC_RSI14: 0.0139\n",
      "   - WMT_EMA_12: 0.0130\n",
      "   - KO_BB_Upper20: 0.0125\n",
      "   - KO_EMA_26: 0.0122\n",
      "   - KO_EMA_12: 0.0116\n",
      "   - COST_SMA_50: 0.0112\n",
      "   - Kalman_Filtered_Close_PG_lag_10: 0.0110\n",
      "-> Running model: CatBoost\n",
      "   Top 10 Features (or Coefficients):\n",
      "   - WMT_vs_PEP_CloseRatio: 5.5777\n",
      "   - WMT_vs_KO_CloseRatio: 4.1197\n",
      "   - WMT_EMA_26: 3.0523\n",
      "   - DGS10: 2.1433\n",
      "   - PG_CMF_21: 2.1003\n",
      "   - KO_BB_Bandwidth20: 1.8951\n",
      "   - ^GSPC_MACD_Signal: 1.8348\n",
      "   - WMT_RollingMean_Convergence_50: 1.8187\n",
      "   - COST_MACD_Signal: 1.7758\n",
      "   - PG_MACD_Line: 1.7726\n",
      "-> Running model: RandomForest\n",
      "   Top 10 Features (or Coefficients):\n",
      "   - WMT_vs_PEP_CloseRatio: 0.0131\n",
      "   - WMT_vs_KO_CloseRatio: 0.0122\n",
      "   - WMT_vs_KO_Spread: 0.0099\n",
      "   - PG_MACD_Signal: 0.0081\n",
      "   - PG_MACD_Line: 0.0079\n",
      "   - ^GSPC_RollingMean_Convergence_50: 0.0078\n",
      "   - WMT_BB_Lower20: 0.0077\n",
      "   - WMT_EMA_12: 0.0076\n",
      "   - WMT_EMA_26: 0.0075\n",
      "   - ^GSPC_CMF_21: 0.0075\n",
      "-> Running model: LogisticRegression\n",
      "   Top 10 Features (or Coefficients):\n",
      "   - Volume_PG: 0.0000\n",
      "   - Volume_COST: 0.0000\n",
      "   - PEP_Volume_MA_20D: 0.0000\n",
      "   - PG_Volume_MA_20D: 0.0000\n",
      "   - COST_Volume_MA_20D: 0.0000\n",
      "   - KO_Volume_MA_20D: 0.0000\n",
      "   - Volume_PEP: 0.0000\n",
      "   - WMT_TrueRange_Vol_Interaction: 0.0000\n",
      "   - PEP_TrueRange_Vol_Interaction: 0.0000\n",
      "   - WMT_Volume_x_ATR: 0.0000\n",
      "\n",
      "--- Starting Model Screening for Window=10, Threshold=0.01 ---\n",
      "Class imbalance ratio (0/1): 1.20\n",
      "-> Running model: XGBoost\n",
      "   Top 10 Features (or Coefficients):\n",
      "   - WMT_vs_^GSPC_ATR_Ratio: 0.0196\n",
      "   - ^GSPC_RollingMean_Convergence_50: 0.0156\n",
      "   - Kalman_Filtered_Close_COST_lag_5: 0.0149\n",
      "   - ^GSPC_BB_Upper20: 0.0141\n",
      "   - High_PEP: 0.0139\n",
      "   - WMT_SMA_10: 0.0129\n",
      "   - ^GSPC_BB_PctB20: 0.0120\n",
      "   - WMT_EMA_12: 0.0117\n",
      "   - KO_SMA_20: 0.0117\n",
      "   - Kalman_Filtered_Close_COST_lag_1: 0.0116\n",
      "-> Running model: CatBoost\n",
      "   Top 10 Features (or Coefficients):\n",
      "   - WMT_vs_PEP_CloseRatio: 5.2147\n",
      "   - WMT_SMA_10: 3.1057\n",
      "   - WMT_vs_KO_CloseRatio: 2.9443\n",
      "   - WMT_ADX_14: 2.6466\n",
      "   - PG_CMF_21: 2.4195\n",
      "   - PG_MACD_Line: 2.1874\n",
      "   - ^GSPC_MACD_Signal: 2.1593\n",
      "   - Kalman_Filtered_Close_WMT_lag_1: 2.0740\n",
      "   - WMT_RollingMean_Convergence_50: 2.0626\n",
      "   - ^GSPC_RollingMean_Convergence_50: 2.0444\n",
      "-> Running model: RandomForest\n",
      "   Top 10 Features (or Coefficients):\n",
      "   - WMT_vs_KO_CloseRatio: 0.0112\n",
      "   - WMT_vs_PEP_CloseRatio: 0.0099\n",
      "   - WMT_EMA_26: 0.0083\n",
      "   - WMT_RollingMean_Convergence_50: 0.0078\n",
      "   - WMT_EMA_12: 0.0078\n",
      "   - WMT_Volume_MA_20D: 0.0077\n",
      "   - WMT_SMA_10: 0.0075\n",
      "   - WMT_BB_Bandwidth20: 0.0075\n",
      "   - Kalman_Filtered_Close_WMT_lag_1: 0.0074\n",
      "   - WMT_vs_KO_Spread: 0.0073\n",
      "-> Running model: LogisticRegression\n",
      "   Top 10 Features (or Coefficients):\n",
      "   - COST_OBV: 0.0000\n",
      "   - WMT_TrueRange_Vol_Interaction: 0.0000\n",
      "   - COST_TrueRange_Vol_Interaction: 0.0000\n",
      "   - WMT_Volume_MA_20D: 0.0000\n",
      "   - PEP_TrueRange_Vol_Interaction: 0.0000\n",
      "   - KO_Volume_MA_20D: 0.0000\n",
      "   - Volume_PEP: 0.0000\n",
      "   - KO_OBV: 0.0000\n",
      "   - PEP_Volume_MA_20D: 0.0000\n",
      "   - Volume_WMT: 0.0000\n"
     ]
    }
   ],
   "source": [
    "# --- 3. THE EXPERIMENTAL LOOP (MODEL SCREENING) ---\n",
    "results = []\n",
    "target_ticker = 'WMT'\n",
    "split_date = '2021-01-01'\n",
    "\n",
    "for window in window_sizes:\n",
    "    for threshold in thresholds:\n",
    "        print(f\"\\n--- Starting Model Screening for Window={window}, Threshold={threshold} ---\")\n",
    "\n",
    "        # A. Dynamically create the target variable for this experiment\n",
    "        data_target = create_target_variable(data.copy(), target_ticker, window=window, threshold=threshold)\n",
    "\n",
    "        # B. Define features (X) and target (y)\n",
    "        # Dynamically get the target column name from the `create_target_variable` function output\n",
    "        target_col_name = f'{target_ticker}_Target'\n",
    "        target_return_col_name = f'{target_ticker}_target_return_{window}D_{threshold}'\n",
    "        \n",
    "        # Define columns to drop to create the feature set (X)\n",
    "        columns_to_drop = [\n",
    "            target_col_name,\n",
    "            target_return_col_name,\n",
    "            f'Open_{target_ticker}',\n",
    "            f'High_{target_ticker}',\n",
    "            f'Low_{target_ticker}',\n",
    "            f'Close_{target_ticker}'\n",
    "        ]\n",
    "\n",
    "        # Create feature (X) and target (y) sets\n",
    "        X = data_target.drop(columns=columns_to_drop, errors='ignore')\n",
    "        y = data_target[target_col_name]\n",
    "\n",
    "        # Replace inf and -inf values with NaN\n",
    "        X.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "        # Fill NaN values with the mean of the column, or 0 if the column is all NaN\n",
    "        X = X.fillna(X.mean()).fillna(0)\n",
    "        \n",
    "        # Split data into training and testing sets based on the split date\n",
    "        X_train = X.loc[:split_date].copy()\n",
    "        y_train = y.loc[:split_date].copy()\n",
    "        X_test = X.loc[split_date:].copy()\n",
    "        y_test = y.loc[split_date:].copy()\n",
    "\n",
    "        # C. Handle Class Imbalance\n",
    "        neg_to_pos_ratio = (y_train == 0).sum() / (y_train == 1).sum()\n",
    "        print(f\"Class imbalance ratio (0/1): {neg_to_pos_ratio:.2f}\")\n",
    "\n",
    "        for exp in experiment_configs:\n",
    "            print(f\"-> Running model: {exp['model_name']}\")\n",
    "            \n",
    "            # Create a new model instance for this run\n",
    "            model = exp['model_class'](**exp['initial_params'])\n",
    "            \n",
    "            # Get the param_grid and add the scale_pos_weight for tree-based models\n",
    "            param_grid = exp['param_grid'].copy()\n",
    "            if exp['model_name'] in ['XGBoost', 'CatBoost']:\n",
    "                param_grid['scale_pos_weight'] = [neg_to_pos_ratio]\n",
    "            # Logistic Regression and RandomForest have `class_weight='balanced'` which handles this internally\n",
    "\n",
    "            # D. Fit with a lightweight GridSearchCV\n",
    "            # Uses a TimeSeriesSplit-like cross-validation (cv=3) for initial tuning\n",
    "            grid_search = GridSearchCV(\n",
    "                estimator=model,\n",
    "                param_grid=param_grid,\n",
    "                scoring='f1_macro', # Use F1 score for a balanced evaluation on imbalanced data\n",
    "                cv=3,\n",
    "                verbose=0,\n",
    "                n_jobs=-1\n",
    "            )\n",
    "            \n",
    "            grid_search.fit(X_train, y_train)\n",
    "            best_model = grid_search.best_estimator_\n",
    "\n",
    "            # E. Evaluate the model and collect results\n",
    "            y_pred = best_model.predict(X_test)\n",
    "            report = classification_report(y_test, y_pred, output_dict=True)\n",
    "            \n",
    "            # F. Extract and store top features\n",
    "            top_features = {}\n",
    "            if hasattr(best_model, 'feature_importances_'):\n",
    "                # For tree-based models like Random Forest, XGBoost, CatBoost\n",
    "                importances = pd.Series(best_model.feature_importances_, index=X_train.columns)\n",
    "                top_features = importances.nlargest(10).to_dict()\n",
    "            elif hasattr(best_model, 'coef_'):\n",
    "                # For linear models like Logistic Regression\n",
    "                coefficients = pd.Series(best_model.coef_[0], index=X_train.columns)\n",
    "                top_features = coefficients.abs().nlargest(10).to_dict()\n",
    "            \n",
    "            print(\"   Top 10 Features (or Coefficients):\")\n",
    "            for feature, score in top_features.items():\n",
    "                print(f\"   - {feature}: {score:.4f}\")\n",
    "            \n",
    "            results.append({\n",
    "                'Model': exp['model_name'],\n",
    "                'Window': window,\n",
    "                'Threshold': threshold,\n",
    "                'Test_Accuracy': report['accuracy'],\n",
    "                'Test_Precision_1': report['1']['precision'],\n",
    "                'Test_Recall_1': report['1']['recall'],\n",
    "                'Test_F1_1': report['1']['f1-score'],\n",
    "                'Best_Params': grid_search.best_params_,\n",
    "                'Top_Features': top_features\n",
    "            })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7c0891bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Final Experiment Results Summary ---\n",
      "                 Model  Window  Threshold  Test_Accuracy  Test_Precision_1  Test_Recall_1  Test_F1_1                                                                                          Best_Params\n",
      "0   LogisticRegression      10      0.005       0.584549          0.584337       0.892638   0.706311                                                                   {'C': 10.0, 'solver': 'liblinear'}\n",
      "1   LogisticRegression      10      0.010       0.540773          0.525057       0.796200   0.632807                                                                   {'C': 10.0, 'solver': 'liblinear'}\n",
      "2   LogisticRegression       5      0.005       0.519313          0.511062       0.796552   0.622642                                                                   {'C': 10.0, 'solver': 'liblinear'}\n",
      "3   LogisticRegression       5      0.010       0.503004          0.437831       0.682474   0.533441                                                                    {'C': 0.1, 'solver': 'liblinear'}\n",
      "4         RandomForest      10      0.005       0.519313          0.600437       0.421779   0.495495                                                               {'max_depth': 10, 'n_estimators': 100}\n",
      "5              XGBoost      10      0.005       0.494421          0.563380       0.429448   0.487380  {'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 100, 'scale_pos_weight': 0.9627611262488647}\n",
      "6              XGBoost      10      0.010       0.523605          0.525316       0.430052   0.472934  {'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 100, 'scale_pos_weight': 1.2039775624681286}\n",
      "7             CatBoost      10      0.005       0.475536          0.543710       0.391104   0.454951      {'depth': 3, 'learning_rate': 0.1, 'n_estimators': 100, 'scale_pos_weight': 0.9627611262488647}\n",
      "8             CatBoost       5      0.005       0.512446          0.513274       0.400000   0.449612      {'depth': 3, 'learning_rate': 0.1, 'n_estimators': 100, 'scale_pos_weight': 1.1502487562189054}\n",
      "9             CatBoost      10      0.010       0.511588          0.512690       0.348877   0.415211      {'depth': 3, 'learning_rate': 0.1, 'n_estimators': 100, 'scale_pos_weight': 1.2039775624681286}\n",
      "10             XGBoost       5      0.005       0.502146          0.500000       0.334483   0.400826  {'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 100, 'scale_pos_weight': 1.1502487562189054}\n",
      "11            CatBoost       5      0.010       0.570815          0.476780       0.317526   0.381188      {'depth': 3, 'learning_rate': 0.1, 'n_estimators': 100, 'scale_pos_weight': 1.5733849359928551}\n",
      "12        RandomForest      10      0.010       0.519313          0.533101       0.264249   0.353349                                                               {'max_depth': 10, 'n_estimators': 100}\n",
      "13        RandomForest       5      0.010       0.556223          0.442857       0.255670   0.324183                                                                {'max_depth': 5, 'n_estimators': 200}\n",
      "14             XGBoost       5      0.010       0.558798          0.447273       0.253608   0.323684  {'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 100, 'scale_pos_weight': 1.5733849359928551}\n",
      "15        RandomForest       5      0.005       0.496996          0.488372       0.217241   0.300716                                                                {'max_depth': 5, 'n_estimators': 100}\n"
     ]
    }
   ],
   "source": [
    "# --- 4. DISPLAY FINAL RESULTS ---\n",
    "# Convert results list to a DataFrame for easy viewing\n",
    "results_df = pd.DataFrame(results)\n",
    "# Sort the results by F1 score in descending order and reset the index\n",
    "results_df = results_df.sort_values(by='Test_F1_1', ascending=False).reset_index(drop=True)\n",
    "results_df.drop(columns=['Top_Features'], inplace=True) # Drop top features for cleaner output\n",
    "\n",
    "print(\"\\n--- Final Experiment Results Summary ---\")\n",
    "print(results_df.to_string())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

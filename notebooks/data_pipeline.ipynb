{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ca51a41f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Starting Data Preparation Pipeline ---\n",
      "Fetching data for 6 tickers from 2010-01-01 to 2023-01-01...\n",
      "\n",
      "Discovered stock prefixes: ['COST', 'KO', 'PEP', 'PG', 'WMT', '^GSPC']\n",
      "\n",
      "Processing features for stock prefix: COST\n",
      "  - Calculating ATR for COST (window=14)...\n",
      "  - Calculating RSI for COST (window=14)...\n",
      "  - Calculating MACD for COST (fast=12, slow=26, signal=9)...\n",
      "\n",
      "Processing features for stock prefix: KO\n",
      "  - Calculating ATR for KO (window=14)...\n",
      "  - Calculating RSI for KO (window=14)...\n",
      "  - Calculating MACD for KO (fast=12, slow=26, signal=9)...\n",
      "\n",
      "Processing features for stock prefix: PEP\n",
      "  - Calculating ATR for PEP (window=14)...\n",
      "  - Calculating RSI for PEP (window=14)...\n",
      "  - Calculating MACD for PEP (fast=12, slow=26, signal=9)...\n",
      "\n",
      "Processing features for stock prefix: PG\n",
      "  - Calculating ATR for PG (window=14)...\n",
      "  - Calculating RSI for PG (window=14)...\n",
      "  - Calculating MACD for PG (fast=12, slow=26, signal=9)...\n",
      "\n",
      "Processing features for stock prefix: WMT\n",
      "  - Calculating ATR for WMT (window=14)...\n",
      "  - Calculating RSI for WMT (window=14)...\n",
      "  - Calculating MACD for WMT (fast=12, slow=26, signal=9)...\n",
      "\n",
      "Processing features for stock prefix: ^GSPC\n",
      "  - Calculating ATR for ^GSPC (window=14)...\n",
      "  - Calculating RSI for ^GSPC (window=14)...\n",
      "  - Calculating MACD for ^GSPC (fast=12, slow=26, signal=9)...\n",
      "\n",
      "Applying general features and targets...\n",
      "  - Adding lagged features...\n",
      "\n",
      "Dropped 49 rows due to NaN values after feature engineering.\n",
      "\n",
      "--- Data Preparation Complete ---\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 26\u001b[39m\n\u001b[32m     24\u001b[39m tickers_to_filter = tickers_list\n\u001b[32m     25\u001b[39m lags_to_use = [\u001b[32m1\u001b[39m, \u001b[32m5\u001b[39m, \u001b[32m10\u001b[39m]\n\u001b[32m---> \u001b[39m\u001b[32m26\u001b[39m consumer_stocks_df = \u001b[43mapply_kalman_filter_with_lag\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconsumer_stocks_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtickers_to_filter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlags_to_use\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\epoch_bpjmdqk\\Documents\\Code\\src\\stock_features.py:726\u001b[39m, in \u001b[36mapply_kalman_filter_with_lag\u001b[39m\u001b[34m(data_df, target_tickers, lags)\u001b[39m\n\u001b[32m    718\u001b[39m \u001b[38;5;66;03m# Initialize and run the Kalman filter\u001b[39;00m\n\u001b[32m    719\u001b[39m kf = KalmanFilter(transition_matrices=[\u001b[32m1\u001b[39m],\n\u001b[32m    720\u001b[39m                   observation_matrices=[\u001b[32m1\u001b[39m],\n\u001b[32m    721\u001b[39m                   initial_state_mean=measurements[\u001b[32m0\u001b[39m],\n\u001b[32m    722\u001b[39m                   initial_state_covariance=\u001b[32m1\u001b[39m,\n\u001b[32m    723\u001b[39m                   observation_covariance=\u001b[32m1\u001b[39m,\n\u001b[32m    724\u001b[39m                   transition_covariance=\u001b[32m0.01\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m726\u001b[39m (filtered_state_means, filtered_state_covariances) = \u001b[43mkf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfilter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmeasurements\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    727\u001b[39m filtered_prices = filtered_state_means.flatten()\n\u001b[32m    729\u001b[39m \u001b[38;5;66;03m# Add new lagged features for each specified lag\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\epoch_bpjmdqk\\Documents\\Code\\venv\\Lib\\site-packages\\pykalman\\standard.py:1223\u001b[39m, in \u001b[36mKalmanFilter.filter\u001b[39m\u001b[34m(self, X)\u001b[39m\n\u001b[32m   1210\u001b[39m Z = \u001b[38;5;28mself\u001b[39m._parse_observations(X)\n\u001b[32m   1212\u001b[39m (\n\u001b[32m   1213\u001b[39m     transition_matrices,\n\u001b[32m   1214\u001b[39m     transition_offsets,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1220\u001b[39m     initial_state_covariance,\n\u001b[32m   1221\u001b[39m ) = \u001b[38;5;28mself\u001b[39m._initialize_parameters()\n\u001b[32m-> \u001b[39m\u001b[32m1223\u001b[39m (_, _, _, filtered_state_means, filtered_state_covariances) = \u001b[43m_filter\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1224\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtransition_matrices\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1225\u001b[39m \u001b[43m    \u001b[49m\u001b[43mobservation_matrices\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1226\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtransition_covariance\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1227\u001b[39m \u001b[43m    \u001b[49m\u001b[43mobservation_covariance\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1228\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtransition_offsets\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1229\u001b[39m \u001b[43m    \u001b[49m\u001b[43mobservation_offsets\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1230\u001b[39m \u001b[43m    \u001b[49m\u001b[43minitial_state_mean\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1231\u001b[39m \u001b[43m    \u001b[49m\u001b[43minitial_state_covariance\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1232\u001b[39m \u001b[43m    \u001b[49m\u001b[43mZ\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1233\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1234\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m (filtered_state_means, filtered_state_covariances)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\epoch_bpjmdqk\\Documents\\Code\\venv\\Lib\\site-packages\\pykalman\\standard.py:402\u001b[39m, in \u001b[36m_filter\u001b[39m\u001b[34m(transition_matrices, observation_matrices, transition_covariance, observation_covariance, transition_offsets, observation_offsets, initial_state_mean, initial_state_covariance, observations)\u001b[39m\n\u001b[32m    396\u001b[39m     observation_covariance = _last_dims(observation_covariance, t)\n\u001b[32m    397\u001b[39m     observation_offset = _last_dims(observation_offsets, t, ndims=\u001b[32m1\u001b[39m)\n\u001b[32m    398\u001b[39m     (\n\u001b[32m    399\u001b[39m         kalman_gains[t],\n\u001b[32m    400\u001b[39m         filtered_state_means[t],\n\u001b[32m    401\u001b[39m         filtered_state_covariances[t],\n\u001b[32m--> \u001b[39m\u001b[32m402\u001b[39m     ) = \u001b[43m_filter_correct\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    403\u001b[39m \u001b[43m        \u001b[49m\u001b[43mobservation_matrix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    404\u001b[39m \u001b[43m        \u001b[49m\u001b[43mobservation_covariance\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    405\u001b[39m \u001b[43m        \u001b[49m\u001b[43mobservation_offset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    406\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpredicted_state_means\u001b[49m\u001b[43m[\u001b[49m\u001b[43mt\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    407\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpredicted_state_covariances\u001b[49m\u001b[43m[\u001b[49m\u001b[43mt\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    408\u001b[39m \u001b[43m        \u001b[49m\u001b[43mobservations\u001b[49m\u001b[43m[\u001b[49m\u001b[43mt\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    409\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    411\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[32m    412\u001b[39m     predicted_state_means,\n\u001b[32m    413\u001b[39m     predicted_state_covariances,\n\u001b[32m   (...)\u001b[39m\u001b[32m    416\u001b[39m     filtered_state_covariances,\n\u001b[32m    417\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\epoch_bpjmdqk\\Documents\\Code\\venv\\Lib\\site-packages\\pykalman\\standard.py:286\u001b[39m, in \u001b[36m_filter_correct\u001b[39m\u001b[34m(observation_matrix, observation_covariance, observation_offset, predicted_state_mean, predicted_state_covariance, observation)\u001b[39m\n\u001b[32m    275\u001b[39m predicted_observation_mean = (\n\u001b[32m    276\u001b[39m     np.dot(observation_matrix, predicted_state_mean) + observation_offset\n\u001b[32m    277\u001b[39m )\n\u001b[32m    278\u001b[39m predicted_observation_covariance = (\n\u001b[32m    279\u001b[39m     np.dot(\n\u001b[32m    280\u001b[39m         observation_matrix,\n\u001b[32m   (...)\u001b[39m\u001b[32m    283\u001b[39m     + observation_covariance\n\u001b[32m    284\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m286\u001b[39m kalman_gain = \u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    287\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpredicted_state_covariance\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    288\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobservation_matrix\u001b[49m\u001b[43m.\u001b[49m\u001b[43mT\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlinalg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpinv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpredicted_observation_covariance\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    289\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    291\u001b[39m corrected_state_mean = predicted_state_mean + np.dot(\n\u001b[32m    292\u001b[39m     kalman_gain, observation - predicted_observation_mean\n\u001b[32m    293\u001b[39m )\n\u001b[32m    294\u001b[39m corrected_state_covariance = predicted_state_covariance - np.dot(\n\u001b[32m    295\u001b[39m     kalman_gain, np.dot(observation_matrix, predicted_state_covariance)\n\u001b[32m    296\u001b[39m )\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "# Add the parent directory (where 'src' folder is located) to sys.path\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..')))\n",
    "from src.stock_features import prepare_data_for_ml, apply_kalman_filter_with_lag\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Define the parameters for your data pipeline\n",
    "tickers_list = ['PG', 'KO', 'PEP', 'WMT', 'COST', '^GSPC']\n",
    "start_date_str = '2010-01-01'\n",
    "end_date_str = '2023-01-01'\n",
    "output_filename = \"consumer_stocks.csv\"\n",
    "\n",
    "# Make the single function call to run the entire pipeline\n",
    "consumer_stocks_df = prepare_data_for_ml(\n",
    "    tickers=tickers_list,\n",
    "    start_date=start_date_str,\n",
    "    end_date=end_date_str,\n",
    "    #output_engineered_csv=f'data/processed/{output_filename}'\n",
    ")\n",
    "\n",
    "# Apply Kalman filter with lag\n",
    "tickers_to_filter = tickers_list\n",
    "lags_to_use = [1, 5, 10]\n",
    "consumer_stocks_df = apply_kalman_filter_with_lag(consumer_stocks_df, tickers_to_filter, lags_to_use)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "873ffc61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "DatetimeIndex: 3223 entries, 2010-03-16 to 2022-12-30\n",
      "Columns: 308 entries, Close_COST to Kalman_Filtered_Close_^GSPC_lag_10\n",
      "dtypes: float64(302), int64(6)\n",
      "memory usage: 7.6 MB\n"
     ]
    }
   ],
   "source": [
    "consumer_stocks_df.head(3)\n",
    "consumer_stocks_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcf3108e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting FRED data orchestration pipeline...\n",
      "Fetching and processing data for: CPI (CPIAUCSL)\n",
      "Fetching and processing data for: RETAIL_SALES (RSAFS)\n",
      "Fetching and processing data for: PAYEMS (PAYEMS)\n",
      "Fetching and processing data for: REAL_GDP (GDPC1)\n",
      "Fetching and processing data for: UNEMPLOYMENT (UNRATE)\n",
      "Fetching and processing data for: TREASURY_YIELD (DGS10)\n",
      "Fetching and processing data for: FEDERAL_FUNDS_RATE (DFF)\n",
      "Data orchestration complete.\n",
      "\n",
      "--- Final Merged and Cleaned DataFrame ---\n",
      "            CPIAUCSL     RSAFS    PAYEMS      GDPC1  UNRATE  DGS10   DFF\n",
      "date                                                                    \n",
      "1992-01-01     138.3  159177.0  108365.0  10236.435     7.3   6.71  4.09\n",
      "1992-01-02     138.3  159177.0  108365.0  10236.435     7.3   6.78  4.61\n",
      "1992-01-03     138.3  159177.0  108365.0  10236.435     7.3   6.85  4.06\n",
      "1992-01-04     138.3  159177.0  108365.0  10236.435     7.3   6.85  4.06\n",
      "1992-01-05     138.3  159177.0  108365.0  10236.435     7.3   6.85  4.06\n",
      "\n",
      "Final DataFrame info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "DatetimeIndex: 12145 entries, 1992-01-01 to 2025-04-01\n",
      "Freq: D\n",
      "Data columns (total 7 columns):\n",
      " #   Column    Non-Null Count  Dtype  \n",
      "---  ------    --------------  -----  \n",
      " 0   CPIAUCSL  12145 non-null  float64\n",
      " 1   RSAFS     12145 non-null  float64\n",
      " 2   PAYEMS    12145 non-null  float64\n",
      " 3   GDPC1     12145 non-null  float64\n",
      " 4   UNRATE    12145 non-null  float64\n",
      " 5   DGS10     12145 non-null  float64\n",
      " 6   DFF       12145 non-null  float64\n",
      "dtypes: float64(7)\n",
      "memory usage: 759.1 KB\n",
      "None\n",
      "\n",
      "Final DataFrame NaN count:\n",
      "CPIAUCSL    0\n",
      "RSAFS       0\n",
      "PAYEMS      0\n",
      "GDPC1       0\n",
      "UNRATE      0\n",
      "DGS10       0\n",
      "DFF         0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..')))\n",
    "from src.macro_features import FRED_fetch_macro_data, macro_data_orchestrator\n",
    "import pandas as pd\n",
    "\n",
    "FRED_series_ids = {\n",
    "        'CPI': 'CPIAUCSL',\n",
    "        'FEDERAL_FUNDS_RATE': 'DFF',\n",
    "        'TREASURY_YIELD': 'DGS10',\n",
    "        'UNEMPLOYMENT': 'UNRATE',\n",
    "        'REAL_GDP': 'GDPC1',\n",
    "        'RETAIL_SALES': 'RSAFS',\n",
    "        'PAYEMS': 'PAYEMS' \n",
    "    }\n",
    "\n",
    "macro_funcs = { 'CPI', 'FEDERAL_FUNDS_RATE', 'TREASURY_YIELD', \n",
    "                'UNEMPLOYMENT', 'REAL_GDP', 'RETAIL_SALES', 'PAYEMS' }\n",
    "\n",
    "# Specify a start date to test the new functionality\n",
    "#START_DATE = '2010-01-01'\n",
    "\n",
    "# Now you pass the dictionary explicitly as an argument.\n",
    "combined_macro_df = macro_data_orchestrator(\n",
    "    macro_funcs_to_fetch=macro_funcs,\n",
    "    fred_series_ids_dict=FRED_series_ids,\n",
    ")\n",
    "\n",
    "if not combined_macro_df.empty:\n",
    "    print(\"\\n--- Final Merged and Cleaned DataFrame ---\")\n",
    "    print(combined_macro_df.head())\n",
    "    print(\"\\nFinal DataFrame info:\")\n",
    "    print(combined_macro_df.info())\n",
    "    print(\"\\nFinal DataFrame NaN count:\")\n",
    "    print(combined_macro_df.isna().sum())\n",
    "else:\n",
    "    print(\"Orchestrator returned an empty DataFrame.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7acf0d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Merge the two DataFrames on their date index\n",
    "# The how='left' argument keeps all rows from the stock DataFrame.\n",
    "merged_df = pd.merge(\n",
    "    consumer_stocks_df,\n",
    "    combined_macro_df,\n",
    "    left_index=True,\n",
    "    right_index=True,\n",
    "    how='left'\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b608345c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the merged DataFrame to a CSV file\n",
    "output_file = r'C:\\Users\\epoch_bpjmdqk\\Documents\\Code\\data\\processed\\stock_and_macro.csv'\n",
    "merged_df.to_csv(output_file, index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "321647fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters for a more granular news sentiment search\n",
    "# sentiment_symbol = 'WMT'\n",
    "# sentiment_topics = ['retail_and_wholesale'] \n",
    "# # Use the same date range as your stock data for backtesting\n",
    "# sentiment_start_date = '20230101T0000' # YYYYMMDDTHHMM\n",
    "# sentiment_end_date = '20230801T0000'\n",
    "\n",
    "# print(\"\\n--- Fetching More Granular News Sentiment Data ---\")\n",
    "# news_df = fetch_news_sentiment(\n",
    "#     symbol=sentiment_symbol,\n",
    "#     topics=sentiment_topics,\n",
    "#     sort_by='RELEVANCE',\n",
    "#     time_from=sentiment_start_date,\n",
    "#     time_to=sentiment_end_date\n",
    "# )\n",
    "\n",
    "# news_df.info()\n",
    "# print(news_df.head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

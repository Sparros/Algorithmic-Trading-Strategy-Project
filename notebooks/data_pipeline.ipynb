{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca51a41f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "# Add the parent directory (where 'src' folder is located) to sys.path\n",
    "# sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..')))\n",
    "from src.stock_features import build_stock_features_orchestrator, build_sector_base_features, make_target_view \n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# --- Pathing: make src importable no matter where you run the script/notebook ---\n",
    "PROJECT_ROOT = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "if PROJECT_ROOT not in sys.path:\n",
    "    sys.path.append(PROJECT_ROOT)\n",
    "\n",
    "from src.stock_features import (\n",
    "    build_stock_features_orchestrator,\n",
    "    build_sector_base_features,\n",
    "    make_target_view\n",
    ")\n",
    "\n",
    "# NEW: lean on your macro helpers instead of a manual merge\n",
    "from src.macro_features import (\n",
    "    macro_data_orchestrator,\n",
    "    normalize_date_col,\n",
    "    prepare_macro_for_daily_merge,\n",
    "    merge_stocks_and_macros,\n",
    ")\n",
    "\n",
    "# -------- Output folder --------\n",
    "base_output_dir = r\"C:\\Users\\epoch_bpjmdqk\\Documents\\Code\\data\\processed\"\n",
    "macro_folder = r\"C:\\Users\\epoch_bpjmdqk\\Documents\\Code\\data\\raw\"\n",
    "os.makedirs(base_output_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00248952",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------- Sector definitions --------\n",
    "SECTORS = {\n",
    "    \"staples\": {\n",
    "        \"tickers\": [\"WMT\",\"PG\",\"KO\",\"PEP\",\"COST\",\"CL\",\"CLX\",\"KMB\",\"GIS\",\"MDLZ\",\"KR\",\"TGT\",\"XLP\",\"^GSPC\"],\n",
    "        \"sector_etf\": \"XLP\",\n",
    "    },\n",
    "    \"discretionary\": {\n",
    "        \"tickers\": [\"AMZN\",\"HD\",\"MCD\",\"NKE\",\"SBUX\",\"TJX\",\"LOW\",\"BKNG\",\"ROST\",\"MAR\",\"XLY\",\"^GSPC\"],\n",
    "        \"sector_etf\": \"XLY\",\n",
    "    },\n",
    "    \"healthcare\": {\n",
    "        \"tickers\": [\"UNH\",\"LLY\",\"JNJ\",\"ABBV\",\"MRK\",\"TMO\",\"ABT\",\"PFE\",\"MDT\",\"ISRG\",\"CVS\",\"HUM\",\"XLV\",\"^GSPC\"],\n",
    "        \"sector_etf\": \"XLV\",\n",
    "    },\n",
    "    \"technology\": {\n",
    "        \"tickers\": [\"AAPL\",\"MSFT\",\"NVDA\",\"AVGO\",\"ADBE\",\"CRM\",\"AMD\",\"INTC\",\"CSCO\",\"QCOM\",\"ORCL\",\"TXN\",\"XLK\",\"^GSPC\"],\n",
    "        \"sector_etf\": \"XLK\",\n",
    "    },\n",
    "    \"financials\": {\n",
    "        \"tickers\": [\"JPM\",\"BAC\",\"WFC\",\"MS\",\"GS\",\"C\",\"BLK\",\"PGR\",\"AXP\",\"USB\",\"SCHW\",\"CB\",\"XLF\",\"^GSPC\"],\n",
    "        \"sector_etf\": \"XLF\",\n",
    "    },\n",
    "    \"energy\": {\n",
    "        \"tickers\": [\"XOM\",\"CVX\",\"COP\",\"EOG\",\"SLB\",\"OXY\",\"PSX\",\"MPC\",\"VLO\",\"HAL\",\"KMI\",\"XLE\",\"^GSPC\"],\n",
    "        \"sector_etf\": \"XLE\",\n",
    "    },\n",
    "    \"industrials\": {\n",
    "        \"tickers\": [\"CAT\",\"BA\",\"HON\",\"GE\",\"UPS\",\"UNP\",\"DE\",\"RTX\",\"LMT\",\"ETN\",\"EMR\",\"MMM\",\"XLI\",\"^GSPC\"],\n",
    "        \"sector_etf\": \"XLI\",\n",
    "    },\n",
    "    \"utilities\": {\n",
    "        \"tickers\": [\"NEE\",\"SO\",\"DUK\",\"AEP\",\"EXC\",\"SRE\",\"XEL\",\"D\",\"PEG\",\"ED\",\"XLU\",\"^GSPC\"],\n",
    "        \"sector_etf\": \"XLU\",\n",
    "    },\n",
    "    \"materials\": {\n",
    "        \"tickers\": [\"LIN\",\"APD\",\"ECL\",\"NEM\",\"FCX\",\"NUE\",\"SHW\",\"ALB\",\"MLM\",\"VMC\",\"XLB\",\"^GSPC\"],\n",
    "        \"sector_etf\": \"XLB\",\n",
    "    },\n",
    "    \"communication_services\": {\n",
    "        \"tickers\": [\"META\",\"GOOGL\",\"GOOG\",\"NFLX\",\"CMCSA\",\"DIS\",\"T\",\"VZ\",\"XLC\",\"^GSPC\"],\n",
    "        \"sector_etf\": \"XLC\",\n",
    "    },\n",
    "    \"real_estate\": {\n",
    "        \"tickers\": [\"AMT\",\"PLD\",\"EQIX\",\"PSA\",\"SPG\",\"CCI\",\"O\",\"WELL\",\"XLRE\",\"^GSPC\"],\n",
    "        \"sector_etf\": \"XLRE\",\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b8a2868",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------- Data quality helper --------------------\n",
    "def validate_df(df: pd.DataFrame, name: str, require_dtindex: bool = True) -> pd.DataFrame:\n",
    "    \"\"\"Basic QA: datetime index, monotonic, no duplicates, finite values, minimal all-NaN cols.\"\"\"\n",
    "    if df is None or len(df) == 0:\n",
    "        print(f\"[validate] {name}: empty dataframe\")\n",
    "        return df\n",
    "\n",
    "    # Ensure DateTimeIndex\n",
    "    if require_dtindex and not isinstance(df.index, pd.DatetimeIndex):\n",
    "        try:\n",
    "            # Try to coerce if a 'date' column exists\n",
    "            if 'date' in df.columns:\n",
    "                df['date'] = pd.to_datetime(df['date'], utc=False, errors='coerce')\n",
    "                df = df.set_index('date')\n",
    "            else:\n",
    "                df.index = pd.to_datetime(df.index, utc=False, errors='coerce')\n",
    "        except Exception as e:\n",
    "            print(f\"[validate] {name}: failed to coerce datetime index ({e})\")\n",
    "\n",
    "    # Drop timezone\n",
    "    if getattr(df.index, \"tz\", None) is not None:\n",
    "        df.index = df.index.tz_localize(None)\n",
    "\n",
    "    # Sort index + drop duplicates\n",
    "    before = len(df)\n",
    "    df = df[~df.index.duplicated(keep='first')].sort_index()\n",
    "    if len(df) != before:\n",
    "        print(f\"[validate] {name}: removed {before - len(df)} duplicate index rows\")\n",
    "\n",
    "    # Replace infs\n",
    "    inf_mask = ~np.isfinite(df.select_dtypes(include=[np.number]))\n",
    "    if inf_mask.values.any():\n",
    "        n_infs = int(inf_mask.values.sum())\n",
    "        df = df.replace([np.inf, -np.inf], np.nan)\n",
    "        print(f\"[validate] {name}: replaced {n_infs} Â±inf with NaN\")\n",
    "\n",
    "    # Drop columns that are entirely NaN\n",
    "    all_na_cols = [c for c in df.columns if df[c].isna().all()]\n",
    "    if all_na_cols:\n",
    "        print(f\"[validate] {name}: dropping all-NaN columns: {all_na_cols}\")\n",
    "        df = df.drop(columns=all_na_cols)\n",
    "\n",
    "    # Optional: enforce monotonic index\n",
    "    if not df.index.is_monotonic_increasing:\n",
    "        df = df.sort_index()\n",
    "        print(f\"[validate] {name}: index sorted to be monotonic increasing\")\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcf3108e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.macro_features import macro_data_orchestrator, normalize_date_col, prepare_macro_for_daily_merge, merge_stocks_and_macros\n",
    "# (optional) date range\n",
    "start_date_str = None  \n",
    "end_date_str   = None  \n",
    "\n",
    "FRED_series_ids = {\n",
    "        'CPI': 'CPIAUCSL',\n",
    "        'FEDERAL_FUNDS_RATE': 'DFF',\n",
    "        'TREASURY_YIELD': 'DGS10',\n",
    "        'UNEMPLOYMENT': 'UNRATE',\n",
    "        'REAL_GDP': 'GDPC1',\n",
    "        'RETAIL_SALES': 'RSAFS',\n",
    "        'PAYEMS': 'PAYEMS' \n",
    "    }\n",
    "\n",
    "macro_funcs = { 'CPI', 'FEDERAL_FUNDS_RATE', 'TREASURY_YIELD', \n",
    "                'UNEMPLOYMENT', 'REAL_GDP', 'RETAIL_SALES', 'PAYEMS' }\n",
    "\n",
    "# Try load macro data, if no macro data run orchestraotr:\n",
    "try:\n",
    "    macro_df = pd.read_csv(\n",
    "        r'C:\\Users\\epoch_bpjmdqk\\Documents\\Code\\data\\raw\\macros.csv'\n",
    "    )\n",
    "    print(\"Loaded existing macro data from CSV.\")\n",
    "    print(f\"Data loaded: {macro_df.shape}\")\n",
    "except FileNotFoundError:\n",
    "    macro_df = macro_data_orchestrator(\n",
    "        macro_funcs_to_fetch=macro_funcs,\n",
    "        fred_series_ids_dict=FRED_series_ids,\n",
    "        start_date=start_date_str,\n",
    "        save_path=macro_folder,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7acf0d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------- Build sector datasets --------\n",
    "for sector_name, cfg in SECTORS.items():\n",
    "    tickers = cfg[\"tickers\"]\n",
    "    sector_etf = cfg.get(\"sector_etf\")\n",
    "\n",
    "    # 1) build or load BASE once\n",
    "    base_path = os.path.join(base_output_dir, f\"{sector_name}__BASE.parquet\")\n",
    "    if os.path.exists(base_path):\n",
    "        base_df = pd.read_parquet(base_path)\n",
    "        print(f\"[cache] loaded {base_path}\")\n",
    "    else:\n",
    "        base_df = build_sector_base_features(\n",
    "            tickers=tickers,\n",
    "            kalman_lags=[1,5,10],\n",
    "            dropna_frac=0.90,\n",
    "            output_path=base_path\n",
    "        )\n",
    "    if getattr(base_df.index, \"tz\", None) is not None:\n",
    "        base_df.index = base_df.index.tz_localize(None)\n",
    "\n",
    "    equities = [t for t in tickers if not t.startswith(\"^\") and t != sector_etf]\n",
    "\n",
    "    # 2) cheap target views\n",
    "    for target in equities:\n",
    "        suppliers = [t for t in equities if t != target]\n",
    "        print(f\"\\n--- {sector_name} :: target={target} ---\")\n",
    "        df_t = make_target_view(base_df, target_ticker=target, supplier_tickers=suppliers,\n",
    "                                benchmark_ticker=\"^GSPC\", sector_etf=sector_etf)\n",
    "\n",
    "        # merge macro (shift 1d to avoid lookahead)\n",
    "        if not macro_df.empty:\n",
    "            merged = pd.merge(df_t, macro_df, left_index=True, right_index=True, how='left')\n",
    "            macro_cols = list(macro_df.columns)\n",
    "            merged[macro_cols] = merged[macro_cols].shift(1)\n",
    "        else:\n",
    "            merged = df_t\n",
    "\n",
    "        out_path = os.path.join(base_output_dir, f\"{sector_name}__{target}.csv\")\n",
    "        merged.to_csv(out_path, index=True)\n",
    "        print(f\"Saved {sector_name}::{target} â {out_path} rows={len(merged):,} cols={merged.shape[1]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

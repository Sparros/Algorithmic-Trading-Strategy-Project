{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1f471a31",
   "metadata": {},
   "source": [
    "### Data Pipeline — Sector Builds with Macro As-Of Merge, Rolling Transforms, and Manifests\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca51a41f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "\n",
    "from datetime import datetime\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..')))\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Transform utilities (assumed to exist in your repo)\n",
    "from src.transforms.macro_asof import AsOfMacroMerger\n",
    "from src.transforms.rolling import RollingImputer, RollingStandardizer, Winsorizer\n",
    "from src.transforms.pruning import drop_sparse_columns\n",
    "from src.transforms.manifests import ManifestWriter, hash_df_map\n",
    "\n",
    "# QA helpers (defined below in src/qa/)\n",
    "from src.qa.validators import validate_df, assert_no_future, assert_monotonic\n",
    "\n",
    "PUBLISH_LAGS = {\n",
    "    \"CPIAUCSL\": 2, \"DFF\": 1, \"DGS10\": 1,\n",
    "    \"UNRATE\": 2, \"GDPC1\": 5, \"RSAFS\": 3, \"PAYEMS\": 2\n",
    "}\n",
    "\n",
    "from src.stock_features import (\n",
    "    build_stock_features_orchestrator,\n",
    "    build_sector_base_features,\n",
    "    make_target_view\n",
    ")\n",
    "\n",
    "from src.macro_features import (\n",
    "    macro_data_orchestrator,\n",
    "    normalize_date_col,\n",
    "    prepare_macro_for_daily_merge,\n",
    "    merge_stocks_and_macros,\n",
    ")\n",
    "\n",
    "# -------- Config --------\n",
    "base_output_dir = r\"C:\\Users\\epoch_bpjmdqk\\Documents\\Code\\data\\processed\"\n",
    "macro_folder    = r\"C:\\Users\\epoch_bpjmdqk\\Documents\\Code\\data\\raw\"\n",
    "os.makedirs(base_output_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "00248952",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------- Sector definitions --------\n",
    "SECTORS = {\n",
    "    \"staples\": {\n",
    "        \"tickers\": [\"WMT\",\"PG\",\"KO\",\"PEP\",\"COST\",\"CL\",\"CLX\",\"KMB\",\"GIS\",\"MDLZ\",\"KR\",\"TGT\",\"XLP\",\"^GSPC\"],\n",
    "        \"sector_etf\": \"XLP\",\n",
    "    },\n",
    "    \"discretionary\": {\n",
    "        \"tickers\": [\"AMZN\",\"HD\",\"MCD\",\"NKE\",\"SBUX\",\"TJX\",\"LOW\",\"BKNG\",\"ROST\",\"MAR\",\"XLY\",\"^GSPC\"],\n",
    "        \"sector_etf\": \"XLY\",\n",
    "    },\n",
    "    \"healthcare\": {\n",
    "        \"tickers\": [\"UNH\",\"LLY\",\"JNJ\",\"ABBV\",\"MRK\",\"TMO\",\"ABT\",\"PFE\",\"MDT\",\"ISRG\",\"CVS\",\"HUM\",\"XLV\",\"^GSPC\"],\n",
    "        \"sector_etf\": \"XLV\",\n",
    "    },\n",
    "    \"technology\": {\n",
    "        \"tickers\": [\"AAPL\",\"MSFT\",\"NVDA\",\"AVGO\",\"ADBE\",\"CRM\",\"AMD\",\"INTC\",\"CSCO\",\"QCOM\",\"ORCL\",\"TXN\",\"XLK\",\"^GSPC\"],\n",
    "        \"sector_etf\": \"XLK\",\n",
    "    },\n",
    "    \"financials\": {\n",
    "        \"tickers\": [\"JPM\",\"BAC\",\"WFC\",\"MS\",\"GS\",\"C\",\"BLK\",\"PGR\",\"AXP\",\"USB\",\"SCHW\",\"CB\",\"XLF\",\"^GSPC\"],\n",
    "        \"sector_etf\": \"XLF\",\n",
    "    },\n",
    "    \"energy\": {\n",
    "        \"tickers\": [\"XOM\",\"CVX\",\"COP\",\"EOG\",\"SLB\",\"OXY\",\"PSX\",\"MPC\",\"VLO\",\"HAL\",\"KMI\",\"XLE\",\"^GSPC\"],\n",
    "        \"sector_etf\": \"XLE\",\n",
    "    },\n",
    "    \"industrials\": {\n",
    "        \"tickers\": [\"CAT\",\"BA\",\"HON\",\"GE\",\"UPS\",\"UNP\",\"DE\",\"RTX\",\"LMT\",\"ETN\",\"EMR\",\"MMM\",\"XLI\",\"^GSPC\"],\n",
    "        \"sector_etf\": \"XLI\",\n",
    "    },\n",
    "    \"utilities\": {\n",
    "        \"tickers\": [\"NEE\",\"SO\",\"DUK\",\"AEP\",\"EXC\",\"SRE\",\"XEL\",\"D\",\"PEG\",\"ED\",\"XLU\",\"^GSPC\"],\n",
    "        \"sector_etf\": \"XLU\",\n",
    "    },\n",
    "    \"materials\": {\n",
    "        \"tickers\": [\"LIN\",\"APD\",\"ECL\",\"NEM\",\"FCX\",\"NUE\",\"SHW\",\"ALB\",\"MLM\",\"VMC\",\"XLB\",\"^GSPC\"],\n",
    "        \"sector_etf\": \"XLB\",\n",
    "    },\n",
    "    \"communication_services\": {\n",
    "        \"tickers\": [\"META\",\"GOOGL\",\"GOOG\",\"NFLX\",\"CMCSA\",\"DIS\",\"T\",\"VZ\",\"XLC\",\"^GSPC\"],\n",
    "        \"sector_etf\": \"XLC\",\n",
    "    },\n",
    "    \"real_estate\": {\n",
    "        \"tickers\": [\"AMT\",\"PLD\",\"EQIX\",\"PSA\",\"SPG\",\"CCI\",\"O\",\"WELL\",\"XLRE\",\"^GSPC\"],\n",
    "        \"sector_etf\": \"XLRE\",\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b8a2868",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (optional) date range\n",
    "start_date_str = None  \n",
    "end_date_str   = None  \n",
    "\n",
    "FRED_series_ids = {\n",
    "    'CPI': 'CPIAUCSL',\n",
    "    'FEDERAL_FUNDS_RATE': 'DFF',\n",
    "    'TREASURY_YIELD': 'DGS10',\n",
    "    'UNEMPLOYMENT': 'UNRATE',\n",
    "    'REAL_GDP': 'GDPC1',\n",
    "    'RETAIL_SALES': 'RSAFS',\n",
    "    'PAYEMS': 'PAYEMS'\n",
    "}\n",
    "\n",
    "macro_funcs = {'CPI','FEDERAL_FUNDS_RATE','TREASURY_YIELD','UNEMPLOYMENT','REAL_GDP','RETAIL_SALES','PAYEMS'}\n",
    "\n",
    "# ---- Load / build macro data ----\n",
    "try:\n",
    "    macro_df = pd.read_csv(os.path.join(macro_folder, 'macros.csv'))\n",
    "    print(\"Loaded existing macro data from CSV.\")\n",
    "except FileNotFoundError:\n",
    "    macro_df = macro_data_orchestrator(macro_funcs_to_fetch=macro_funcs,\n",
    "                                       fred_series_ids_dict=FRED_series_ids,\n",
    "                                       start_date=start_date_str,\n",
    "                                       save_path=macro_folder)\n",
    "\n",
    "macro_df    = normalize_date_col(macro_df, col=\"Date\")\n",
    "macro_daily = prepare_macro_for_daily_merge(macro_df)\n",
    "\n",
    "# Apply series-specific publish lags to mimic availability\n",
    "merger = AsOfMacroMerger(lags=PUBLISH_LAGS)\n",
    "macro_daily = merger.apply(macro_daily)\n",
    "\n",
    "# Validate macro\n",
    "macro_daily = validate_df(macro_daily.set_index(\"Date\"), \"macro_daily\")\n",
    "\n",
    "# append macro state/regime features (PCA→KMeans, curve, proxies, surprises)\n",
    "from src.macro_features import build_macro_state_features\n",
    "macro_daily = build_macro_state_features(macro_daily, publish_lags=PUBLISH_LAGS, pca_cols=None, use_hmm=False)\n",
    "\n",
    "# Re-validate\n",
    "macro_daily = validate_df(macro_daily, \"macro_daily+state\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcf3108e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded existing macro data from CSV.\n",
      "Data loaded: (31652, 8)\n",
      "[validate] macro_daily: replaced 42934 ±inf with NaN\n"
     ]
    }
   ],
   "source": [
    "# ---- Sector loop ----\n",
    "for sector_name, cfg in SECTORS.items():\n",
    "    tickers    = cfg[\"tickers\"]\n",
    "    sector_etf = cfg.get(\"sector_etf\")\n",
    "\n",
    "    # 1) Base features per sector (cacheable)\n",
    "    base_path = os.path.join(base_output_dir, f\"{sector_name}__BASE.parquet\")\n",
    "    if os.path.exists(base_path):\n",
    "        base_df = pd.read_parquet(base_path)\n",
    "        print(f\"[cache] loaded {base_path}\")\n",
    "    else:\n",
    "        base_df = build_sector_base_features(\n",
    "            tickers=tickers,\n",
    "            kalman_lags=[1,5,10],\n",
    "            dropna_frac=0.90,\n",
    "            output_path=base_path\n",
    "        )\n",
    "    base_df = validate_df(base_df, f\"{sector_name}::BASE\")\n",
    "    base_df = drop_sparse_columns(base_df, 0.20, f\"{sector_name}::BASE\")\n",
    "\n",
    "    # 2) Target views + macro merge\n",
    "    equities = [t for t in tickers if not t.startswith(\"^\") and t != sector_etf]\n",
    "    for target in equities:\n",
    "        suppliers = [t for t in equities if t != target]\n",
    "        print(f\"\\n--- {sector_name} :: target={target} ---\")\n",
    "\n",
    "        df_t = make_target_view(\n",
    "            base_df,\n",
    "            target_ticker=target,\n",
    "            supplier_tickers=suppliers,\n",
    "            benchmark_ticker=\"^GSPC\",\n",
    "            sector_etf=sector_etf\n",
    "        )\n",
    "        df_t = validate_df(df_t, f\"{sector_name}::{target}::target_view\")\n",
    "\n",
    "        # As-of safe merge with macro\n",
    "        df_t_for_merge   = df_t.reset_index().rename(columns={df_t.index.name or \"index\": \"Date\"})\n",
    "        macro_for_merge  = macro_daily.reset_index()\n",
    "        merged           = merge_stocks_and_macros(stock_df=df_t_for_merge, macro_df=macro_for_merge, tolerance_days=31)\n",
    "\n",
    "        merged = merged.set_index(\"Date\")\n",
    "        merged = validate_df(merged, f\"{sector_name}::{target}::merged\")\n",
    "        merged = drop_sparse_columns(merged, 0.20, f\"{sector_name}::{target}::merged\")\n",
    "\n",
    "        # ---- Fast QA guards (fail fast) ----\n",
    "        assert_monotonic(merged.index)\n",
    "        assert_no_future(merged.index, macro_daily.index)\n",
    "        if merged.index.duplicated().any():\n",
    "            raise ValueError(f\"[{sector_name}::{target}] Duplicate index rows detected\")\n",
    "\n",
    "        # Warm-up trim (indicators, rolling) and row-level NA filter\n",
    "        if len(merged) > 60:\n",
    "            merged = merged.iloc[60:]\n",
    "            print(f\"[finalize] {sector_name}::{target}: trimmed first 60 warm-up rows\")\n",
    "\n",
    "        min_non_null = int(0.85 * merged.shape[1])\n",
    "        merged = merged.loc[merged.notna().sum(axis=1) >= min_non_null]\n",
    "\n",
    "        # 3) Rolling, no-lookahead transforms\n",
    "        pipe = Pipeline([\n",
    "            (\"winsor\", Winsorizer(clip=5.0)),           # clip each feature by robust quantiles\n",
    "            (\"imputer\", RollingImputer(window=20)),     # ffill/bfill + rolling mean fallback\n",
    "            (\"scaler\", RollingStandardizer(window=60))  # rolling z-score\n",
    "        ])\n",
    "        merged_proc = pipe.fit_transform(merged)\n",
    "\n",
    "        # 4) Manifest logging (data hash map)\n",
    "        manifest = ManifestWriter(base_output_dir)\n",
    "        manifest.write(sector_name, target, hash_df_map(merged_proc))\n",
    "\n",
    "        # 5) Save\n",
    "        out_path = os.path.join(base_output_dir, f\"{sector_name}__{target}.csv\")\n",
    "        merged_proc.to_csv(out_path, index=True)\n",
    "        print(f\"Saved {sector_name}::{target} → {out_path} rows={len(merged_proc):,} cols={merged_proc.shape[1]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

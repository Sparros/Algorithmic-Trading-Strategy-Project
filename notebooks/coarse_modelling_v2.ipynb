{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a4e4ddbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.metrics import classification_report, mean_squared_error, f1_score\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "from xgboost import XGBClassifier, XGBRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from catboost import CatBoostClassifier, CatBoostRegressor\n",
    "from collections import defaultdict\n",
    "from sklearn.impute import SimpleImputer\n",
    "import pickle\n",
    "\n",
    "# Add parent directory to path to access custom modules\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..')))\n",
    "from src.stock_features import create_target_variable\n",
    "from src.modelling_functions import create_multi_class_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "453c99eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. Load Data ---\n",
    "try:\n",
    "    data = pd.read_csv(r'C:\\Users\\epoch_bpjmdqk\\Documents\\Code\\data\\processed\\consumer_staples_data.csv', index_col='Date', parse_dates=True)\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: The data file was not found. Please update the file path.\")\n",
    "    sys.exit()\n",
    "\n",
    "# Define the target and split date\n",
    "target_ticker = 'WMT'\n",
    "# Split date set earlier to ensure test set is not empty with longer windows\n",
    "split_date = '2023-01-01'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4c0d563f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 2. DEFINE THE EXPERIMENTS ---\n",
    "# Define feature engineering configurations\n",
    "feature_configs = {\n",
    "    'baseline': None,\n",
    "    'poly_2': {'name': 'PolynomialFeatures', 'params': {'degree': 2, 'include_bias': False}},\n",
    "    'pca_3': {'name': 'PCA', 'params': {'n_components': 3}},\n",
    "}\n",
    "\n",
    "# Define target configurations\n",
    "target_configs = [\n",
    "    {'type': 'binary', 'window': 5, 'threshold': 0.005},\n",
    "    {'type': 'multi_class', 'window': 5, 'threshold': 0.01},\n",
    "    {'type': 'regression', 'window': 5},\n",
    "]\n",
    "\n",
    "# Define model configurations for classification and regression\n",
    "models_cls = [\n",
    "    {'name': 'XGBoost', 'class': XGBClassifier, 'params': {'eval_metric': 'logloss', 'random_state': 42, 'n_estimators': 100}},\n",
    "    {'name': 'CatBoost', 'class': CatBoostClassifier, 'params': {'verbose': False, 'random_state': 42, 'n_estimators': 100}},\n",
    "    {'name': 'RandomForest', 'class': RandomForestClassifier, 'params': {'random_state': 42, 'n_estimators': 100}},\n",
    "]\n",
    "\n",
    "models_reg = [\n",
    "    {'name': 'XGBoost', 'class': XGBRegressor, 'params': {'objective': 'reg:squarederror', 'random_state': 42, 'n_estimators': 100}},\n",
    "    {'name': 'LinearRegression', 'class': LinearRegression, 'params': {}},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fad8c107",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Starting Experiment: Target=binary, Features=baseline ---\n",
      "-> Running model: XGBoost\n",
      "-> Running model: CatBoost\n",
      "-> Running model: RandomForest\n",
      "\n",
      "--- Starting Experiment: Target=binary, Features=poly_2 ---\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input X contains infinity or a value too large for dtype('float64').",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 53\u001b[39m\n\u001b[32m     49\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m feat_name == \u001b[33m'\u001b[39m\u001b[33mpoly_2\u001b[39m\u001b[33m'\u001b[39m:\n\u001b[32m     50\u001b[39m     \u001b[38;5;66;03m# Apply imputer and then the polynomial features\u001b[39;00m\n\u001b[32m     51\u001b[39m     \u001b[38;5;66;03m# The imputer learns from the training data and applies to both\u001b[39;00m\n\u001b[32m     52\u001b[39m     X_train_imputed = pd.DataFrame(imputer.fit_transform(X_train_full))\n\u001b[32m---> \u001b[39m\u001b[32m53\u001b[39m     X_test_imputed = pd.DataFrame(\u001b[43mimputer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_test_full\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m     55\u001b[39m     transformer = PolynomialFeatures(**feat_config[\u001b[33m'\u001b[39m\u001b[33mparams\u001b[39m\u001b[33m'\u001b[39m])\n\u001b[32m     56\u001b[39m     X_train_transformed = pd.DataFrame(transformer.fit_transform(X_train_imputed))\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\epoch_bpjmdqk\\Documents\\Code\\venv\\Lib\\site-packages\\sklearn\\utils\\_set_output.py:316\u001b[39m, in \u001b[36m_wrap_method_output.<locals>.wrapped\u001b[39m\u001b[34m(self, X, *args, **kwargs)\u001b[39m\n\u001b[32m    314\u001b[39m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[32m    315\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, *args, **kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m316\u001b[39m     data_to_wrap = \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    317\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[32m    318\u001b[39m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[32m    319\u001b[39m         return_tuple = (\n\u001b[32m    320\u001b[39m             _wrap_data_with_container(method, data_to_wrap[\u001b[32m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[32m    321\u001b[39m             *data_to_wrap[\u001b[32m1\u001b[39m:],\n\u001b[32m    322\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\epoch_bpjmdqk\\Documents\\Code\\venv\\Lib\\site-packages\\sklearn\\impute\\_base.py:609\u001b[39m, in \u001b[36mSimpleImputer.transform\u001b[39m\u001b[34m(self, X)\u001b[39m\n\u001b[32m    594\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Impute all missing values in `X`.\u001b[39;00m\n\u001b[32m    595\u001b[39m \n\u001b[32m    596\u001b[39m \u001b[33;03mParameters\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    605\u001b[39m \u001b[33;03m    `X` with imputed values.\u001b[39;00m\n\u001b[32m    606\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    607\u001b[39m check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m609\u001b[39m X = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_validate_input\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43min_fit\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    610\u001b[39m statistics = \u001b[38;5;28mself\u001b[39m.statistics_\n\u001b[32m    612\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m X.shape[\u001b[32m1\u001b[39m] != statistics.shape[\u001b[32m0\u001b[39m]:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\epoch_bpjmdqk\\Documents\\Code\\venv\\Lib\\site-packages\\sklearn\\impute\\_base.py:363\u001b[39m, in \u001b[36mSimpleImputer._validate_input\u001b[39m\u001b[34m(self, X, in_fit)\u001b[39m\n\u001b[32m    361\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m new_ve \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    362\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m363\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m ve\n\u001b[32m    365\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m in_fit:\n\u001b[32m    366\u001b[39m     \u001b[38;5;66;03m# Use the dtype seen in `fit` for non-`fit` conversion\u001b[39;00m\n\u001b[32m    367\u001b[39m     \u001b[38;5;28mself\u001b[39m._fit_dtype = X.dtype\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\epoch_bpjmdqk\\Documents\\Code\\venv\\Lib\\site-packages\\sklearn\\impute\\_base.py:344\u001b[39m, in \u001b[36mSimpleImputer._validate_input\u001b[39m\u001b[34m(self, X, in_fit)\u001b[39m\n\u001b[32m    341\u001b[39m     ensure_all_finite = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    343\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m344\u001b[39m     X = \u001b[43mvalidate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    345\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    346\u001b[39m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    347\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreset\u001b[49m\u001b[43m=\u001b[49m\u001b[43min_fit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    348\u001b[39m \u001b[43m        \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcsc\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    349\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    350\u001b[39m \u001b[43m        \u001b[49m\u001b[43mforce_writeable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43min_fit\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    351\u001b[39m \u001b[43m        \u001b[49m\u001b[43mensure_all_finite\u001b[49m\u001b[43m=\u001b[49m\u001b[43mensure_all_finite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    352\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    353\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    354\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m ve:\n\u001b[32m    355\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mcould not convert\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(ve):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\epoch_bpjmdqk\\Documents\\Code\\venv\\Lib\\site-packages\\sklearn\\utils\\validation.py:2954\u001b[39m, in \u001b[36mvalidate_data\u001b[39m\u001b[34m(_estimator, X, y, reset, validate_separately, skip_check_array, **check_params)\u001b[39m\n\u001b[32m   2952\u001b[39m         out = X, y\n\u001b[32m   2953\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m no_val_y:\n\u001b[32m-> \u001b[39m\u001b[32m2954\u001b[39m     out = \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_name\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mX\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mcheck_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2955\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_y:\n\u001b[32m   2956\u001b[39m     out = _check_y(y, **check_params)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\epoch_bpjmdqk\\Documents\\Code\\venv\\Lib\\site-packages\\sklearn\\utils\\validation.py:1105\u001b[39m, in \u001b[36mcheck_array\u001b[39m\u001b[34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_all_finite, ensure_non_negative, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[39m\n\u001b[32m   1099\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1100\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFound array with dim \u001b[39m\u001b[38;5;132;01m{\u001b[39;00marray.ndim\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m,\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1101\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m while dim <= 2 is required\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcontext\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1102\u001b[39m     )\n\u001b[32m   1104\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m ensure_all_finite:\n\u001b[32m-> \u001b[39m\u001b[32m1105\u001b[39m     \u001b[43m_assert_all_finite\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1106\u001b[39m \u001b[43m        \u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1107\u001b[39m \u001b[43m        \u001b[49m\u001b[43minput_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1108\u001b[39m \u001b[43m        \u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1109\u001b[39m \u001b[43m        \u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[43m=\u001b[49m\u001b[43mensure_all_finite\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mallow-nan\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1110\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1112\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m copy:\n\u001b[32m   1113\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m _is_numpy_namespace(xp):\n\u001b[32m   1114\u001b[39m         \u001b[38;5;66;03m# only make a copy if `array` and `array_orig` may share memory`\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\epoch_bpjmdqk\\Documents\\Code\\venv\\Lib\\site-packages\\sklearn\\utils\\validation.py:120\u001b[39m, in \u001b[36m_assert_all_finite\u001b[39m\u001b[34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001b[39m\n\u001b[32m    117\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m first_pass_isfinite:\n\u001b[32m    118\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m120\u001b[39m \u001b[43m_assert_all_finite_element_wise\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    121\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    122\u001b[39m \u001b[43m    \u001b[49m\u001b[43mxp\u001b[49m\u001b[43m=\u001b[49m\u001b[43mxp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    123\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[43m=\u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    124\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmsg_dtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmsg_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    125\u001b[39m \u001b[43m    \u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    126\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    127\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\epoch_bpjmdqk\\Documents\\Code\\venv\\Lib\\site-packages\\sklearn\\utils\\validation.py:169\u001b[39m, in \u001b[36m_assert_all_finite_element_wise\u001b[39m\u001b[34m(X, xp, allow_nan, msg_dtype, estimator_name, input_name)\u001b[39m\n\u001b[32m    152\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m estimator_name \u001b[38;5;129;01mand\u001b[39;00m input_name == \u001b[33m\"\u001b[39m\u001b[33mX\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m has_nan_error:\n\u001b[32m    153\u001b[39m     \u001b[38;5;66;03m# Improve the error message on how to handle missing values in\u001b[39;00m\n\u001b[32m    154\u001b[39m     \u001b[38;5;66;03m# scikit-learn.\u001b[39;00m\n\u001b[32m    155\u001b[39m     msg_err += (\n\u001b[32m    156\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m does not accept missing values\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    157\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m encoded as NaN natively. For supervised learning, you might want\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m    167\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m#estimators-that-handle-nan-values\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    168\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m169\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg_err)\n",
      "\u001b[31mValueError\u001b[39m: Input X contains infinity or a value too large for dtype('float64')."
     ]
    }
   ],
   "source": [
    "# --- 3. THE EXPERIMENTAL LOOP ---\n",
    "all_results = []\n",
    "\n",
    "for target_conf in target_configs:\n",
    "    # A. Create the target variable\n",
    "    data_target = data.copy()\n",
    "    \n",
    "    if target_conf['type'] == 'binary':\n",
    "        data_target = create_target_variable(data_target, target_ticker, window=target_conf['window'], threshold=target_conf['threshold'])\n",
    "        target_col = f'{target_ticker}_Target'\n",
    "        model_list = models_cls\n",
    "    elif target_conf['type'] == 'multi_class':\n",
    "        # Custom multi-class target creation\n",
    "        # (You need to implement this function)\n",
    "        data_target = create_multi_class_target(data_target, target_ticker, window=target_conf['window'], threshold=target_conf['threshold'])\n",
    "        target_col = f'{target_ticker}_Target_Multi'\n",
    "        model_list = models_cls\n",
    "    else: # regression\n",
    "        data_target[f'{target_ticker}_target_return'] = data_target[f'Close_{target_ticker}'].pct_change(periods=target_conf['window']).shift(-target_conf['window'])\n",
    "        target_col = f'{target_ticker}_target_return'\n",
    "        model_list = models_reg\n",
    "\n",
    "    # Drop rows with NaN in the target and drop target-related columns\n",
    "    data_target.dropna(subset=[target_col], inplace=True)\n",
    "    columns_to_drop = [col for col in data_target.columns if target_ticker in col and 'Target' not in col]\n",
    "    X_full = data_target.drop(columns=columns_to_drop, errors='ignore')\n",
    "    y_full = data_target[target_col]\n",
    "\n",
    "    # Split data chronologically\n",
    "    X_train_full = X_full.loc[:split_date]\n",
    "    y_train_full = y_full.loc[:split_date]\n",
    "    X_test_full = X_full.loc[split_date:]\n",
    "    y_test_full = y_full.loc[split_date:]\n",
    "\n",
    "    # B. Loop through feature engineering configurations\n",
    "for feat_name, feat_config in feature_configs.items():\n",
    "    print(f\"\\n--- Starting Experiment: Target={target_conf['type']}, Features={feat_name} ---\")\n",
    "\n",
    "    # Create a copy to avoid side effects\n",
    "    X_train_transformed = X_train_full.copy()\n",
    "    X_test_transformed = X_test_full.copy()\n",
    "\n",
    "    # CRITICAL: Convert inf to nan before imputation\n",
    "    X_train_transformed.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "    X_test_transformed.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "\n",
    "    # D. Apply feature engineering inside the loop to prevent leakage\n",
    "    if feat_name != 'baseline':\n",
    "        # Create an imputer to handle NaNs and Infs before other transformations\n",
    "        # Using 'mean' as the strategy, you could also use 'median'\n",
    "        imputer = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "\n",
    "        if feat_name == 'poly_2':\n",
    "            # Apply imputer and then the polynomial features\n",
    "            # The imputer learns from the training data and applies to both\n",
    "            X_train_imputed = pd.DataFrame(imputer.fit_transform(X_train_transformed))\n",
    "            X_test_imputed = pd.DataFrame(imputer.transform(X_test_transformed))\n",
    "            \n",
    "            transformer = PolynomialFeatures(**feat_config['params'])\n",
    "            X_train_transformed = pd.DataFrame(transformer.fit_transform(X_train_imputed))\n",
    "            X_test_transformed = pd.DataFrame(transformer.transform(X_test_imputed))\n",
    "\n",
    "        elif feat_name == 'pca_3':\n",
    "            # Apply imputer, then standard scaler, then PCA\n",
    "            imputer = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "            X_train_imputed = pd.DataFrame(imputer.fit_transform(X_train_transformed))\n",
    "            X_test_imputed = pd.DataFrame(imputer.transform(X_test_transformed))\n",
    "            \n",
    "            scaler = StandardScaler()\n",
    "            X_train_scaled = scaler.fit_transform(X_train_imputed)\n",
    "            X_test_scaled = scaler.transform(X_test_imputed)\n",
    "\n",
    "            transformer = PCA(**feat_config['params'])\n",
    "            X_train_transformed = pd.DataFrame(transformer.fit_transform(X_train_scaled))\n",
    "            X_test_transformed = pd.DataFrame(transformer.transform(X_test_scaled))\n",
    "\n",
    "        # E. Loop through models\n",
    "        for model_conf in model_list:\n",
    "            print(f\"-> Running model: {model_conf['name']}\")\n",
    "            \n",
    "            # F. Apply walk-forward validation (on the training data)\n",
    "            tscv = TimeSeriesSplit(n_splits=5)\n",
    "            cv_results = defaultdict(list)\n",
    "            \n",
    "            for fold, (train_idx, val_idx) in enumerate(tscv.split(X_train_transformed)):\n",
    "                X_train, y_train = X_train_transformed.iloc[train_idx], y_train_full.iloc[train_idx]\n",
    "                X_val, y_val = X_train_transformed.iloc[val_idx], y_train_full.iloc[val_idx]\n",
    "                \n",
    "                model = model_conf['class'](**model_conf['params'])\n",
    "                \n",
    "                if model_conf['name'] in ['XGBoost', 'CatBoost'] and target_conf['type'] in ['binary', 'multi_class']:\n",
    "                    neg_to_pos_ratio = (y_train == 0).sum() / (y_train == 1).sum()\n",
    "                    model.set_params(scale_pos_weight=neg_to_pos_ratio)\n",
    "                \n",
    "                model.fit(X_train, y_train)\n",
    "                y_pred = model.predict(X_val)\n",
    "                \n",
    "                if target_conf['type'] == 'regression':\n",
    "                    cv_results['rmse'].append(np.sqrt(mean_squared_error(y_val, y_pred)))\n",
    "                else:\n",
    "                    cv_results['f1'].append(f1_score(y_val, y_pred, average='macro', zero_division=0))\n",
    "            \n",
    "            # G. Store results\n",
    "            avg_performance = np.mean(cv_results['f1']) if 'f1' in cv_results else np.mean(cv_results['rmse'])\n",
    "            all_results.append({\n",
    "                'Feature_Set': feat_name,\n",
    "                'Target_Type': target_conf['type'],\n",
    "                'Window': target_conf['window'],\n",
    "                'Model': model_conf['name'],\n",
    "                'CV_Performance': avg_performance,\n",
    "                'Model_Params': model_conf['params']\n",
    "            })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbfa367c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 4. Display Final Summary ---\n",
    "results_df = pd.DataFrame(all_results)\n",
    "results_df['CV_Performance'] = results_df['CV_Performance'].round(4)\n",
    "print(\"\\n--- Final Coarse Modeling Results Summary ---\")\n",
    "print(results_df.sort_values(by='CV_Performance', ascending=False).to_string())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

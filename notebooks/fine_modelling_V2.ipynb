{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ed2c7bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from sklearn.model_selection import GridSearchCV, TimeSeriesSplit\n",
    "from sklearn.metrics import classification_report, f1_score, precision_score, recall_score, accuracy_score\n",
    "from xgboost import XGBClassifier\n",
    "from collections import defaultdict\n",
    "from itertools import product\n",
    "\n",
    "# Add parent directory to path to access custom modules\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..')))\n",
    "from src.stock_features import create_target_variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9324e243",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Data setup for Window=5, Threshold=0.005 ---\n",
      "Class imbalance ratio (0/1): 1.20\n"
     ]
    }
   ],
   "source": [
    "# --- 1. DATA PREPARATION ---\n",
    "# Load the prepared data\n",
    "try:\n",
    "    data = pd.read_csv(r'C:\\Users\\epoch_bpjmdqk\\Documents\\Code\\data\\processed\\consumer_staples_data.csv', index_col='Date', parse_dates=True)\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: The data file was not found. Please update the file path.\")\n",
    "    sys.exit()\n",
    "\n",
    "# Define the optimal data preparation parameters from the coarse modeling stage\n",
    "window = 5\n",
    "threshold = 0.005\n",
    "target_ticker = 'WMT'\n",
    "split_date = '2021-01-01'\n",
    "\n",
    "# Create the target variable based on the selected parameters\n",
    "data_target = create_target_variable(data.copy(), target_ticker, window=window, threshold=threshold)\n",
    "\n",
    "# Define columns to drop to create the feature set (X)\n",
    "target_col_name = f'{target_ticker}_Target'\n",
    "target_return_col_name = f'{target_ticker}_target_return_{window}D_{threshold}'\n",
    "columns_to_drop = [\n",
    "    target_col_name,\n",
    "    target_return_col_name,\n",
    "    f'Open_{target_ticker}',\n",
    "    f'High_{target_ticker}',\n",
    "    f'Low_{target_ticker}',\n",
    "    f'Close_{target_ticker}'\n",
    "]\n",
    "\n",
    "# Handle NaN values and split the data\n",
    "data_target.dropna(inplace=True)\n",
    "# Filter for 'pca_3' features, as this was the top-performing feature set\n",
    "features_pca_3 = [col for col in data_target.columns if 'PCA' in col]\n",
    "X = data_target[features_pca_3]\n",
    "y = data_target[target_col_name]\n",
    "\n",
    "# Create a fixed training and testing set for the final evaluation\n",
    "X_train_full = X.loc[:split_date].copy()\n",
    "y_train_full = y.loc[:split_date].copy()\n",
    "X_test_full = X.loc[split_date:].copy()\n",
    "y_test_full = y.loc[split_date:].copy()\n",
    "\n",
    "neg_to_pos_ratio = (y_train_full == 0).sum() / (y_train_full == 1).sum()\n",
    "print(f\"\\n--- Data setup for Window={window}, Threshold={threshold} ---\")\n",
    "print(f\"Class imbalance ratio (0/1): {neg_to_pos_ratio:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7d7f2aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 2. DEFINE REFINED XGBOOST EXPERIMENT CONFIGURATION ---\n",
    "# We use the results from the coarse search to narrow down the hyperparameter space.\n",
    "# Focusing exclusively on XGBoost with a refined parameter grid.\n",
    "refined_experiment_config = {\n",
    "    'model_name': 'XGBoost',\n",
    "    'model_class': XGBClassifier,\n",
    "    'initial_params': {'eval_metric': 'logloss', 'use_label_encoder': False, 'random_state': 42, 'scale_pos_weight': neg_to_pos_ratio},\n",
    "    # Refined param grid based on coarse search results and best practices\n",
    "    'param_grid': {\n",
    "        'n_estimators': [100, 200, 300],\n",
    "        'learning_rate': [0.01, 0.05, 0.1, 0.2],\n",
    "        'max_depth': [3, 5, 7],\n",
    "        'subsample': [0.7, 0.8, 0.9, 1.0],\n",
    "        'colsample_bytree': [0.7, 0.8, 0.9, 1.0],\n",
    "        'gamma': [0, 0.1, 0.5]\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dd6cf35b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Starting Walk-Forward Validation for XGBoost ---\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'product' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m--- Starting Walk-Forward Validation for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m ---\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# Generate all possible parameter combinations\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m param_combinations = [\u001b[38;5;28mdict\u001b[39m(\u001b[38;5;28mzip\u001b[39m(param_grid.keys(), v)) \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m \u001b[43mproduct\u001b[49m(*param_grid.values())]\n\u001b[32m     12\u001b[39m \u001b[38;5;66;03m# Define the walk-forward validation settings\u001b[39;00m\n\u001b[32m     13\u001b[39m train_window_size = \u001b[32m100\u001b[39m \u001b[38;5;66;03m# Number of samples in the training window\u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'product' is not defined"
     ]
    }
   ],
   "source": [
    "# --- 3. FINE-TUNING AND WALK-FORWARD VALIDATION ---\n",
    "model_name = refined_experiment_config['model_name']\n",
    "model_class = refined_experiment_config['model_class']\n",
    "initial_params = refined_experiment_config['initial_params']\n",
    "param_grid = refined_experiment_config['param_grid']\n",
    "\n",
    "print(f\"\\n--- Starting Walk-Forward Validation for {model_name} ---\")\n",
    "\n",
    "# Generate all possible parameter combinations\n",
    "param_combinations = [dict(zip(param_grid.keys(), v)) for v in product(*param_grid.values())]\n",
    "\n",
    "# Define the walk-forward validation settings\n",
    "train_window_size = 100 # Number of samples in the training window\n",
    "test_window_size = 20 # Number of samples in the testing window\n",
    "num_walks = 10 # Number of walk-forward steps\n",
    "\n",
    "cv_metrics = defaultdict(lambda: defaultdict(list))\n",
    "\n",
    "# Loop through each walk-forward step\n",
    "for i in range(num_walks):\n",
    "    start_train_index = i * test_window_size\n",
    "    end_train_index = start_train_index + train_window_size\n",
    "    start_test_index = end_train_index\n",
    "    end_test_index = start_test_index + test_window_size\n",
    "    \n",
    "    if end_test_index > len(X_train_full):\n",
    "        break\n",
    "        \n",
    "    X_train_walk = X_train_full.iloc[start_train_index:end_train_index]\n",
    "    y_train_walk = y_train_full.iloc[start_train_index:end_train_index]\n",
    "    X_test_walk = X_train_full.iloc[start_test_index:end_test_index]\n",
    "    y_test_walk = y_train_full.iloc[start_test_index:end_test_index]\n",
    "\n",
    "    print(f\"Walk {i+1}/{num_walks}: Training from index {start_train_index} to {end_train_index}, testing from {start_test_index} to {end_test_index}\")\n",
    "\n",
    "    for params in param_combinations:\n",
    "        # Create and train the model with the current parameters\n",
    "        model = model_class(**initial_params, **params)\n",
    "        model.fit(X_train_walk, y_train_walk)\n",
    "        \n",
    "        # Make predictions and evaluate\n",
    "        y_pred = model.predict(X_test_walk)\n",
    "        \n",
    "        # Store metrics for this parameter combination and walk\n",
    "        cv_metrics[str(params)]['accuracy'].append(accuracy_score(y_test_walk, y_pred))\n",
    "        cv_metrics[str(params)]['f1_macro'].append(f1_score(y_test_walk, y_pred, average='macro', zero_division=0))\n",
    "\n",
    "# Find the best parameters based on average f1_macro score across all walks\n",
    "best_params = None\n",
    "best_avg_f1 = -1\n",
    "for params_str, metrics in cv_metrics.items():\n",
    "    avg_f1 = np.mean(metrics['f1_macro'])\n",
    "    if avg_f1 > best_avg_f1:\n",
    "        best_avg_f1 = avg_f1\n",
    "        best_params = eval(params_str) # eval() is safe here as param_combinations are controlled\n",
    "\n",
    "print(\"\\n--- Walk-Forward Validation Results Summary ---\")\n",
    "print(f\"Best parameters found through walk-forward validation: {best_params}\")\n",
    "print(f\"Average F1-macro score for best parameters: {best_avg_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aacd043",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 4. FINAL MODEL TRAINING AND EVALUATION ---\n",
    "# Train the final model with the best parameters on the full training set\n",
    "print(\"\\n--- Training final model on full training data with best parameters ---\")\n",
    "best_model = model_class(**initial_params, **best_params)\n",
    "best_model.fit(X_train_full, y_train_full)\n",
    "\n",
    "# Evaluate the final best model on the unseen test set\n",
    "y_pred_final = best_model.predict(X_test_full)\n",
    "report_str = classification_report(y_test_full, y_pred_final, zero_division=0)\n",
    "\n",
    "print(f\"\\n--- Final Model Evaluation on Unseen Test Data for {model_name} ---\")\n",
    "print(\"\\n--- Final Classification Report ---\")\n",
    "print(report_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d0b3f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 5. SAVE THE FINAL MODEL ---\n",
    "# Ensure the models directory exists\n",
    "model_dir = \"C:\\\\Users\\\\epoch_bpjmdqk\\\\Documents\\\\Code\\\\models\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the best model using pickle\n",
    "model_filename = f\"{model_dir}\\\\{model_name.lower()}_w{window}_t{threshold}.pkl\"\n",
    "with open(model_filename, 'wb') as file:\n",
    "    pickle.dump(best_model, file)\n",
    "print(f\"\\n✅ Refined model saved as '{model_filename}'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

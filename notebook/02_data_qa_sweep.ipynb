{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "329a4a29",
   "metadata": {},
   "source": [
    "# Dataset Screening — Fast Triage\n",
    "\n",
    "This notebook screens all `{sector}__{ticker}.csv` files for:\n",
    "- Sample depth & continuity\n",
    "- Target viability\n",
    "- Missingness\n",
    "- Feature diversity\n",
    "- **Preliminary signal (fast Pearson IC, with optional Spearman confirm)**\n",
    "- Regime robustness\n",
    "- Leakage / live-feasibility checks\n",
    "\n",
    "It then scores and ranks datasets, and writes `dataset_screen_report.json`.\n",
    "\n",
    "### Speed-up strategies in this version\n",
    "- Two-stage IC: **fast rolling Pearson on winsorized z-scores** over recent history, then **Spearman confirm** only on promoted features (default top 30 or abs(medIC) ≥ 0.01).\n",
    "- Trim feature set before IC (regex + variance + correlation reps).\n",
    "- Reuse the fast pass wherever possible.\n",
    "\n",
    "You can tune the speed/rigor tradeoff in the **Configuration** cell.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd35967d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import glob\n",
    "import time\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..')))\n",
    "\n",
    "from src.modelling_functions import create_target_variable \n",
    "from src.qa.file_checks import qa_one_file \n",
    "\n",
    "# === Paths ===\n",
    "DATA_DIR = r\"C:\\Users\\epoch_bpjmdqk\\Documents\\Code\\data\\processed\"  # where {sector}__{ticker}.csv \n",
    "SAVE_DIR = r\"C:\\Users\\epoch_bpjmdqk\\Documents\\Code\\data\\raw\"  # where to save aligned/processed data\n",
    "\n",
    "QA_RESULTS_CSV = os.path.join(SAVE_DIR, \"qa_sweep_results.csv\")\n",
    "QA_SUMMARY_JSON= os.path.join(SAVE_DIR, \"qa_sweep_results.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72b8317e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_sector_ticker(filename: str):\n",
    "    base = os.path.basename(filename)\n",
    "    name, _ = os.path.splitext(base)\n",
    "    if \"__\" in name:\n",
    "        sector, ticker = name.split(\"__\", 1)\n",
    "    else:\n",
    "        parts = name.split(\"_\")\n",
    "        sector, ticker = parts[0], parts[-1]\n",
    "    return sector, ticker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddedaef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# discover files\n",
    "all_files = sorted(glob.glob(os.path.join(DATA_DIR, \"*.csv\")))\n",
    "total = len(all_files)\n",
    "if total == 0:\n",
    "    raise FileNotFoundError(f\"No CSVs found in {DATA_DIR}\")\n",
    "\n",
    "print(f\"[qa] Found {total} datasets in: {DATA_DIR}\")\n",
    "if total <= 10:\n",
    "    print(\"[qa] Files:\", \", \".join(os.path.basename(f) for f in all_files))\n",
    "else:\n",
    "    head = \", \".join(os.path.basename(f) for f in all_files[:5])\n",
    "    tail = \", \".join(os.path.basename(f) for f in all_files[-3:])\n",
    "    print(f\"[qa] First 5: {head}\")\n",
    "    print(f\"[qa] Last  3: {tail}\")\n",
    "\n",
    "print(\"-\" * 80)\n",
    "results = []\n",
    "ok_count, err_count = 0, 0\n",
    "t0_all = time.time()\n",
    "\n",
    "for i, path in enumerate(all_files, start=1):\n",
    "    ds = os.path.basename(path)\n",
    "    sector, ticker = parse_sector_ticker(ds)\n",
    "\n",
    "    print(f\"[qa {i:>{len(str(total))}}/{total}] START  {ds}  (sector={sector}, ticker={ticker})\", flush=True)\n",
    "    t0 = time.time()\n",
    "    try:\n",
    "        row = qa_one_file(path)\n",
    "        took = time.time() - t0\n",
    "        # normalize row & add extras\n",
    "        row = dict(row)\n",
    "        row.setdefault(\"dataset\", ds)\n",
    "        row.setdefault(\"ok\", True)\n",
    "        row.setdefault(\"error\", \"\")\n",
    "        row[\"sector\"] = sector\n",
    "        row[\"ticker\"] = ticker\n",
    "        row[\"seconds\"] = round(took, 3)\n",
    "\n",
    "        results.append(row)\n",
    "        ok_count += 1 if row.get(\"ok\", True) and not row.get(\"error\") else 0\n",
    "        err_count += 0 if row.get(\"ok\", True) and not row.get(\"error\") else 1\n",
    "\n",
    "        status = \"OK\" if (row.get(\"ok\", True) and not row.get(\"error\")) else \"WARN/ERR\"\n",
    "        print(f\"[qa {i:>{len(str(total))}}/{total}] DONE   {ds}  → {status}  ({took:,.2f}s)\", flush=True)\n",
    "    except Exception as e:\n",
    "        took = time.time() - t0\n",
    "        err = {\"dataset\": ds, \"sector\": sector, \"ticker\": ticker, \"ok\": False,\n",
    "               \"error\": f\"qa_exception: {type(e).__name__}: {e}\", \"seconds\": round(took, 3)}\n",
    "        results.append(err)\n",
    "        err_count += 1\n",
    "        print(f\"[qa {i:>{len(str(total))}}/{total}] ERROR  {ds}  → {err['error']}  ({took:,.2f}s)\", flush=True)\n",
    "\n",
    "elapsed = time.time() - t0_all\n",
    "print(\"-\" * 80)\n",
    "print(f\"[qa] Finished {total} datasets in {elapsed:,.1f}s  ✓ ok={ok_count}  ✖ errors|warnings={err_count}\")\n",
    "\n",
    "# persist\n",
    "qa_df = pd.DataFrame(results)\n",
    "qa_df.to_csv(QA_RESULTS_CSV, index=False)\n",
    "with open(QA_SUMMARY_JSON, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "\n",
    "print(f\"[qa] Wrote CSV  → {QA_RESULTS_CSV}\")\n",
    "print(f\"[qa] Wrote JSON → {QA_SUMMARY_JSON}\")\n",
    "\n",
    "# pretty print a compact table in the notebook/stdout\n",
    "cols_pref = [\n",
    "    \"dataset\",\"ok\",\"rows\",\"cols\",\"nan_overall\",\"n_sparse_cols\",\"n_near_constant\",\n",
    "    \"dup_index_rows\",\"median_gap_days\",\"worst_macro_staleness_days\",\"n_identical_pairs\",\n",
    "    \"seconds\",\"error\"\n",
    "]\n",
    "show_cols = [c for c in cols_pref if c in qa_df.columns]\n",
    "print(\"\\n[qa] Summary (first 25 rows):\")\n",
    "print(qa_df[show_cols].head(25).to_string(index=False))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
